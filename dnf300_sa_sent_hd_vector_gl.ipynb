{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "import keras.layers as kl\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import spacy\n",
    "from keras.utils import to_categorical\n",
    "from spacy.lang.en import English\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.layers import BatchNormalization, Lambda, Concatenate, Dropout, Conv1D, MaxPooling1D, Input, TimeDistributed, Dense, LSTM, RepeatVector, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from AttentionModules import SelfAttention_gl as SelfAttention,CrossAttention_gl as CrossAttention\n",
    "import sys,os\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['authors', 'evidence', 'headline', 'id', 'reason', 'claims', 'type',\n",
       "        'urls'],\n",
       "       dtype='object'),\n",
       " Index(['authors', 'evidence', 'headline', 'id', 'reason', 'type', 'urls'], dtype='object'),\n",
       " 300,\n",
       " 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnf300 = pd.read_json('evaluation_set/deepnofakes/dnf_300/combined_300.json').T\n",
    "dnf_eval = pd.read_json('evaluation_set/deepnofakes/Evaluation_Final_50_V2.json')\n",
    "dnf_eval.columns = ['authors', 'evidence', 'headline', 'id', 'reason', 'claims', 'type', 'urls'] \n",
    "with open('evaluation_set/deepnofakes/dnf_300/cleaned/cleaned_dnf300_sent_array_id.p', 'rb') as fp:\n",
    "    articles = pickle.load(fp)\n",
    "with open('evaluation_set/deepnofakes/dnf_300/cleaned/cleaned_dnf300_sent_vector_array_id.p', 'rb') as fp:\n",
    "    article_vectors = pickle.load(fp)\n",
    "with open('evaluation_set/word_mapping/id_word_mapping.p', 'rb') as fp:\n",
    "    id_word_mapping = pickle.load(fp)\n",
    "dnf_eval.keys(), dnf300.keys(), len(articles.keys()), len(article_vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_splits = 5\n",
    "kf = KFold(n_splits=num_splits)\n",
    "train_batchsize = 32\n",
    "val_batchsize = 32\n",
    "test_batchsize = 50\n",
    "train_steps_per_epoch = 4\n",
    "val_steps_per_epoch = 1\n",
    "epochs = 2000\n",
    "max_sentences = 0\n",
    "for idx in articles.keys():\n",
    "    num = len(articles[idx])\n",
    "    if num>=max_sentences:\n",
    "        max_sentences = num\n",
    "        \n",
    "max_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = sorted(dnf300.headline.unique())\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_titles = sorted(dnf_eval.headline.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = sorted(dnf300.headline.unique())\n",
    "non_test_titles = np.array(list(set(titles)-set(test_titles)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for train_index, val_index in kf.split(non_test_titles):\n",
    "    indices.append([train_index,val_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 150 151 152 153 154 155 156 157\n",
      " 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175\n",
      " 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193\n",
      " 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211\n",
      " 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229\n",
      " 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247\n",
      " 248 249] [100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
      " 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135\n",
      " 136 137 138 139 140 141 142 143 144 145 146 147 148 149]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 50, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_index, val_index = indices[np.random.randint(0,num_splits)]\n",
    "print(train_index,val_index)\n",
    "val_titles = non_test_titles[val_index]\n",
    "train_titles = non_test_titles[train_index]\n",
    "len(train_titles),len(val_titles),len(test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy():\n",
    "    sentencizer = English()\n",
    "    sentencizer.add_pipe(sentencizer.create_pipe('sentencizer'))\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    return sentencizer, nlp\n",
    "sentencizer, nlp = load_spacy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datagen_dnf(batchsize,dataframe,mode):\n",
    "    counter=0\n",
    "    ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "    while True:\n",
    "        if mode=='train':\n",
    "            idx=np.random.choice(train_titles)\n",
    "        elif mode=='val':\n",
    "            idx=np.random.choice(val_titles)\n",
    "        elif mode=='test':\n",
    "            idx=np.random.choice(test_titles)\n",
    "        idx = idx.strip()\n",
    "        \n",
    "            \n",
    "#         cl = dataframe[dataframe.Article==idx]['Claim'].values\n",
    "#         sentences=articles[ar_id]\n",
    "#         print(len(sentences))\n",
    "        if mode=='test':\n",
    "            hd = dnf_eval[dnf_eval.headline==idx]['headline'].values[0].lower()\n",
    "            ar_id = dnf_eval[dnf_eval.headline==idx]['id'].values[0]\n",
    "            cl = dnf_eval[dnf_eval.headline==idx]['claims'].values[0]\n",
    "            ar_claims.append(cl)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                hd = dataframe[dataframe.headline==idx]['headline'].values[0].lower()\n",
    "                ar_id = dataframe[dataframe.headline==idx]['id'].values[0]\n",
    "                ar_claims.append('None')\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print(idx)\n",
    "        sentences = articles[ar_id]\n",
    "        vectors = article_vectors[ar_id]\n",
    "        hds.append(hd)\n",
    "        ar_sentences.append(sentences)\n",
    "#         print(len(sentences))\n",
    "        sents = np.zeros((max_sentences,300))\n",
    "        \n",
    "        sents[:len(vectors)] = vectors\n",
    "        ar_ids.append(ar_id)\n",
    "        ar_sents.append(sents)\n",
    "        hd_nlp = nlp(hd.lower())\n",
    "        hd_nlp = hd_nlp[:50]\n",
    "        head_classes = np.zeros(50, dtype='int')\n",
    "        for i in range(len(hd_nlp)):\n",
    "            head_classes[i] = hd_nlp[i].rank\n",
    "        ar_head_vectors.append(hd_nlp.vector)\n",
    "        ar_head_classes.append(to_categorical(num_classes=20000,y=head_classes))\n",
    "        counter+=1\n",
    "        if counter==batchsize:\n",
    "            inputs = {\n",
    "                'article_id': np.array(ar_ids)\n",
    "                ,'headline': np.array(hds)\n",
    "                ,'sentence_vectors' : np.array(ar_sents)\n",
    "                ,'input_headline_vector': np.array(ar_head_vectors)\n",
    "                ,'claims':np.array(ar_claims)\n",
    "                ,'sentences':np.array(ar_sentences)\n",
    "            }\n",
    "            outputs = {\n",
    "                'headline_token_classes': np.array(ar_head_classes)\n",
    "                ,'output_headline_vector': np.array(ar_head_vectors)\n",
    "            }\n",
    "            yield inputs,outputs\n",
    "            ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "            counter=0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdg = datagen_dnf(train_batchsize,dnf300,mode='train')\n",
    "vdg = datagen_dnf(val_batchsize,dnf300,mode='val')\n",
    "test_dg = datagen_dnf(test_batchsize,dnf300,mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y = next(test_dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x['sentence_vectors'].shape, x['headline_vector'].shape, y['headline_token_classes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 35, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 35, 16)       14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 35, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 35, 32)       1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 35, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 35, 32)       128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 35, 128)      0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 35, 256)      98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 35, 256)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 35, 256)      1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca1 (CrossAttention_gl)         [(None, 35, 256), (1 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca2 (CrossAttention_gl)         [(None, 35, 256), (1 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca3 (CrossAttention_gl)         [(None, 35, 256), (1 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca4 (CrossAttention_gl)         [(None, 35, 256), (1 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 35, 1024)     0           ca1[0][0]                        \n",
      "                                                                 ca2[0][0]                        \n",
      "                                                                 ca3[0][0]                        \n",
      "                                                                 ca4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 35, 1024)     0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 35, 1024)     4096        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 18, 256)      786688      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 9, 256)       196864      conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 5, 256)       196864      conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 3, 256)       196864      conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 3, 256)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 3, 256)       1024        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          131584      global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 512)          2048        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_headline_vector (Dense)  (None, 300)          153900      batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,465,356\n",
      "Trainable params: 2,460,684\n",
      "Non-trainable params: 4,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"2130pt\" viewBox=\"0.00 0.00 1833.00 2130.00\" width=\"1833pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 2126)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-2126 1829,-2126 1829,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140548510655488 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140548510655488</title>\n",
       "<polygon fill=\"none\" points=\"888,-2075.5 888,-2121.5 1223,-2121.5 1223,-2075.5 888,-2075.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"976\" y=\"-2094.8\">sentence_vectors: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"1064,-2075.5 1064,-2121.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1091.5\" y=\"-2106.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1064,-2098.5 1119,-2098.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1091.5\" y=\"-2083.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1119,-2075.5 1119,-2121.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1171\" y=\"-2106.3\">(None, 35, 300)</text>\n",
       "<polyline fill=\"none\" points=\"1119,-2098.5 1223,-2098.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1171\" y=\"-2083.3\">(None, 35, 300)</text>\n",
       "</g>\n",
       "<!-- 140548510655768 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140548510655768</title>\n",
       "<polygon fill=\"none\" points=\"914.5,-1992.5 914.5,-2038.5 1196.5,-2038.5 1196.5,-1992.5 914.5,-1992.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"976\" y=\"-2011.8\">conv1d_1: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1037.5,-1992.5 1037.5,-2038.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1065\" y=\"-2023.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1037.5,-2015.5 1092.5,-2015.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1065\" y=\"-2000.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1092.5,-1992.5 1092.5,-2038.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1144.5\" y=\"-2023.3\">(None, 35, 300)</text>\n",
       "<polyline fill=\"none\" points=\"1092.5,-2015.5 1196.5,-2015.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1144.5\" y=\"-2000.3\">(None, 35, 16)</text>\n",
       "</g>\n",
       "<!-- 140548510655488&#45;&gt;140548510655768 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140548510655488-&gt;140548510655768</title>\n",
       "<path d=\"M1055.5,-2075.37C1055.5,-2067.15 1055.5,-2057.66 1055.5,-2048.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1059,-2048.61 1055.5,-2038.61 1052,-2048.61 1059,-2048.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548510655880 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140548510655880</title>\n",
       "<polygon fill=\"none\" points=\"917,-1909.5 917,-1955.5 1194,-1955.5 1194,-1909.5 917,-1909.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"979.5\" y=\"-1928.8\">dropout_1: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1042,-1909.5 1042,-1955.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1069.5\" y=\"-1940.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1042,-1932.5 1097,-1932.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1069.5\" y=\"-1917.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1097,-1909.5 1097,-1955.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1145.5\" y=\"-1940.3\">(None, 35, 16)</text>\n",
       "<polyline fill=\"none\" points=\"1097,-1932.5 1194,-1932.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1145.5\" y=\"-1917.3\">(None, 35, 16)</text>\n",
       "</g>\n",
       "<!-- 140548510655768&#45;&gt;140548510655880 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140548510655768-&gt;140548510655880</title>\n",
       "<path d=\"M1055.5,-1992.37C1055.5,-1984.15 1055.5,-1974.66 1055.5,-1965.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1059,-1965.61 1055.5,-1955.61 1052,-1965.61 1059,-1965.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548516563320 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140548516563320</title>\n",
       "<polygon fill=\"none\" points=\"918,-1826.5 918,-1872.5 1193,-1872.5 1193,-1826.5 918,-1826.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"979.5\" y=\"-1845.8\">conv1d_2: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1041,-1826.5 1041,-1872.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1068.5\" y=\"-1857.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1041,-1849.5 1096,-1849.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1068.5\" y=\"-1834.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1096,-1826.5 1096,-1872.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1144.5\" y=\"-1857.3\">(None, 35, 16)</text>\n",
       "<polyline fill=\"none\" points=\"1096,-1849.5 1193,-1849.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1144.5\" y=\"-1834.3\">(None, 35, 32)</text>\n",
       "</g>\n",
       "<!-- 140548510655880&#45;&gt;140548516563320 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140548510655880-&gt;140548516563320</title>\n",
       "<path d=\"M1055.5,-1909.37C1055.5,-1901.15 1055.5,-1891.66 1055.5,-1882.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1059,-1882.61 1055.5,-1872.61 1052,-1882.61 1059,-1882.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548510423024 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140548510423024</title>\n",
       "<polygon fill=\"none\" points=\"917,-1743.5 917,-1789.5 1194,-1789.5 1194,-1743.5 917,-1743.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"979.5\" y=\"-1762.8\">dropout_2: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1042,-1743.5 1042,-1789.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1069.5\" y=\"-1774.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1042,-1766.5 1097,-1766.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1069.5\" y=\"-1751.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1097,-1743.5 1097,-1789.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1145.5\" y=\"-1774.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1097,-1766.5 1194,-1766.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1145.5\" y=\"-1751.3\">(None, 35, 32)</text>\n",
       "</g>\n",
       "<!-- 140548516563320&#45;&gt;140548510423024 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140548516563320-&gt;140548510423024</title>\n",
       "<path d=\"M1055.5,-1826.37C1055.5,-1818.15 1055.5,-1808.66 1055.5,-1799.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1059,-1799.61 1055.5,-1789.61 1052,-1799.61 1059,-1799.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548510018640 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140548510018640</title>\n",
       "<polygon fill=\"none\" points=\"849.5,-1660.5 849.5,-1706.5 1261.5,-1706.5 1261.5,-1660.5 849.5,-1660.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"979.5\" y=\"-1679.8\">batch_normalization_1: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"1109.5,-1660.5 1109.5,-1706.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1137\" y=\"-1691.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1109.5,-1683.5 1164.5,-1683.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1137\" y=\"-1668.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1164.5,-1660.5 1164.5,-1706.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1213\" y=\"-1691.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1164.5,-1683.5 1261.5,-1683.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1213\" y=\"-1668.3\">(None, 35, 32)</text>\n",
       "</g>\n",
       "<!-- 140548510423024&#45;&gt;140548510018640 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140548510423024-&gt;140548510018640</title>\n",
       "<path d=\"M1055.5,-1743.37C1055.5,-1735.15 1055.5,-1725.66 1055.5,-1716.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1059,-1716.61 1055.5,-1706.61 1052,-1716.61 1059,-1716.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548510187360 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140548510187360</title>\n",
       "<polygon fill=\"none\" points=\"287,-1577.5 287,-1623.5 658,-1623.5 658,-1577.5 287,-1577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353\" y=\"-1596.8\">sa1: SelfAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"419,-1577.5 419,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"446.5\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"419,-1600.5 474,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"446.5\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"474,-1577.5 474,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"566\" y=\"-1608.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"474,-1600.5 658,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"566\" y=\"-1585.3\">[(None, 35, 32), (35, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140548510018640&#45;&gt;140548510187360 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140548510018640-&gt;140548510187360</title>\n",
       "<path d=\"M896.963,-1660.47C817.741,-1649.47 721.844,-1636.14 641.259,-1624.95\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"641.392,-1621.43 631.006,-1623.52 640.429,-1628.37 641.392,-1621.43\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548352356592 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140548352356592</title>\n",
       "<polygon fill=\"none\" points=\"676,-1577.5 676,-1623.5 1047,-1623.5 1047,-1577.5 676,-1577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"742\" y=\"-1596.8\">sa2: SelfAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"808,-1577.5 808,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"835.5\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"808,-1600.5 863,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"835.5\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"863,-1577.5 863,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"955\" y=\"-1608.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"863,-1600.5 1047,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"955\" y=\"-1585.3\">[(None, 35, 32), (35, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140548510018640&#45;&gt;140548352356592 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140548510018640-&gt;140548352356592</title>\n",
       "<path d=\"M1002.74,-1660.47C978.37,-1650.3 949.25,-1638.14 923.801,-1627.51\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"924.821,-1624.15 914.245,-1623.52 922.124,-1630.6 924.821,-1624.15\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548351307280 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140548351307280</title>\n",
       "<polygon fill=\"none\" points=\"1065,-1577.5 1065,-1623.5 1436,-1623.5 1436,-1577.5 1065,-1577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1131\" y=\"-1596.8\">sa3: SelfAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"1197,-1577.5 1197,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1224.5\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1197,-1600.5 1252,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1224.5\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1252,-1577.5 1252,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1344\" y=\"-1608.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1252,-1600.5 1436,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1344\" y=\"-1585.3\">[(None, 35, 32), (35, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140548510018640&#45;&gt;140548351307280 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140548510018640-&gt;140548351307280</title>\n",
       "<path d=\"M1108.53,-1660.47C1133.14,-1650.25 1162.56,-1638.03 1188.22,-1627.37\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1189.59,-1630.59 1197.48,-1623.52 1186.91,-1624.13 1189.59,-1630.59\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548350573928 -->\n",
       "<g class=\"node\" id=\"node10\"><title>140548350573928</title>\n",
       "<polygon fill=\"none\" points=\"1454,-1577.5 1454,-1623.5 1825,-1623.5 1825,-1577.5 1454,-1577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1520\" y=\"-1596.8\">sa4: SelfAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"1586,-1577.5 1586,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1613.5\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1586,-1600.5 1641,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1613.5\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1641,-1577.5 1641,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1733\" y=\"-1608.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1641,-1600.5 1825,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1733\" y=\"-1585.3\">[(None, 35, 32), (35, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140548510018640&#45;&gt;140548350573928 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140548510018640-&gt;140548350573928</title>\n",
       "<path d=\"M1214.31,-1660.47C1293.67,-1649.47 1389.73,-1636.14 1470.45,-1624.95\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1471.3,-1628.36 1480.72,-1623.52 1470.34,-1621.43 1471.3,-1628.36\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548349353096 -->\n",
       "<g class=\"node\" id=\"node12\"><title>140548349353096</title>\n",
       "<polygon fill=\"none\" points=\"759.5,-1494.5 759.5,-1540.5 1351.5,-1540.5 1351.5,-1494.5 759.5,-1494.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"843.5\" y=\"-1513.8\">concatenate_1: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"927.5,-1494.5 927.5,-1540.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"955\" y=\"-1525.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"927.5,-1517.5 982.5,-1517.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"955\" y=\"-1502.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"982.5,-1494.5 982.5,-1540.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1167\" y=\"-1525.3\">[(None, 35, 32), (None, 35, 32), (None, 35, 32), (None, 35, 32)]</text>\n",
       "<polyline fill=\"none\" points=\"982.5,-1517.5 1351.5,-1517.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1167\" y=\"-1502.3\">(None, 35, 128)</text>\n",
       "</g>\n",
       "<!-- 140548510187360&#45;&gt;140548349353096 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>140548510187360-&gt;140548349353096</title>\n",
       "<path d=\"M631.037,-1577.47C710.259,-1566.47 806.156,-1553.14 886.741,-1541.95\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"887.571,-1545.37 896.994,-1540.52 886.608,-1538.43 887.571,-1545.37\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548352356592&#45;&gt;140548349353096 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>140548352356592-&gt;140548349353096</title>\n",
       "<path d=\"M914.255,-1577.47C938.63,-1567.3 967.75,-1555.14 993.199,-1544.51\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"994.876,-1547.6 1002.76,-1540.52 992.179,-1541.15 994.876,-1547.6\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548351307280&#45;&gt;140548349353096 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>140548351307280-&gt;140548349353096</title>\n",
       "<path d=\"M1197.47,-1577.47C1172.86,-1567.25 1143.44,-1555.03 1117.78,-1544.37\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1119.09,-1541.13 1108.52,-1540.52 1116.41,-1547.59 1119.09,-1541.13\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548350573928&#45;&gt;140548349353096 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>140548350573928-&gt;140548349353096</title>\n",
       "<path d=\"M1480.69,-1577.47C1401.33,-1566.47 1305.27,-1553.14 1224.55,-1541.95\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1224.66,-1538.43 1214.28,-1540.52 1223.7,-1545.36 1224.66,-1538.43\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548510655712 -->\n",
       "<g class=\"node\" id=\"node11\"><title>140548510655712</title>\n",
       "<polygon fill=\"none\" points=\"398,-1494.5 398,-1540.5 741,-1540.5 741,-1494.5 398,-1494.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"500.5\" y=\"-1513.8\">input_headline_vector: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"603,-1494.5 603,-1540.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"630.5\" y=\"-1525.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"603,-1517.5 658,-1517.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"630.5\" y=\"-1502.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"658,-1494.5 658,-1540.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"699.5\" y=\"-1525.3\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"658,-1517.5 741,-1517.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"699.5\" y=\"-1502.3\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 140548348624008 -->\n",
       "<g class=\"node\" id=\"node13\"><title>140548348624008</title>\n",
       "<polygon fill=\"none\" points=\"475.5,-1411.5 475.5,-1457.5 715.5,-1457.5 715.5,-1411.5 475.5,-1411.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"526.5\" y=\"-1430.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"577.5,-1411.5 577.5,-1457.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"605\" y=\"-1442.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"577.5,-1434.5 632.5,-1434.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"605\" y=\"-1419.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"632.5,-1411.5 632.5,-1457.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"674\" y=\"-1442.3\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"632.5,-1434.5 715.5,-1434.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"674\" y=\"-1419.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140548510655712&#45;&gt;140548348624008 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>140548510655712-&gt;140548348624008</title>\n",
       "<path d=\"M576.605,-1494.37C579.299,-1485.97 582.422,-1476.24 585.343,-1467.14\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"588.68,-1468.2 588.404,-1457.61 582.015,-1466.06 588.68,-1468.2\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548352894512 -->\n",
       "<g class=\"node\" id=\"node14\"><title>140548352894512</title>\n",
       "<polygon fill=\"none\" points=\"912.5,-1411.5 912.5,-1457.5 1194.5,-1457.5 1194.5,-1411.5 912.5,-1411.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"974\" y=\"-1430.8\">conv1d_3: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1035.5,-1411.5 1035.5,-1457.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1063\" y=\"-1442.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1035.5,-1434.5 1090.5,-1434.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1063\" y=\"-1419.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1090.5,-1411.5 1090.5,-1457.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1142.5\" y=\"-1442.3\">(None, 35, 128)</text>\n",
       "<polyline fill=\"none\" points=\"1090.5,-1434.5 1194.5,-1434.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1142.5\" y=\"-1419.3\">(None, 35, 256)</text>\n",
       "</g>\n",
       "<!-- 140548349353096&#45;&gt;140548352894512 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>140548349353096-&gt;140548352894512</title>\n",
       "<path d=\"M1054.95,-1494.37C1054.75,-1486.15 1054.52,-1476.66 1054.3,-1467.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1057.79,-1467.52 1054.05,-1457.61 1050.79,-1467.69 1057.79,-1467.52\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548348030928 -->\n",
       "<g class=\"node\" id=\"node15\"><title>140548348030928</title>\n",
       "<polygon fill=\"none\" points=\"464,-1328.5 464,-1374.5 739,-1374.5 739,-1328.5 464,-1328.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"525.5\" y=\"-1347.8\">lambda_1: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"587,-1328.5 587,-1374.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"614.5\" y=\"-1359.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"587,-1351.5 642,-1351.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"614.5\" y=\"-1336.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"642,-1328.5 642,-1374.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"690.5\" y=\"-1359.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"642,-1351.5 739,-1351.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"690.5\" y=\"-1336.3\">(None, 1, 256)</text>\n",
       "</g>\n",
       "<!-- 140548348624008&#45;&gt;140548348030928 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>140548348624008-&gt;140548348030928</title>\n",
       "<path d=\"M597.14,-1411.37C597.748,-1403.15 598.451,-1393.66 599.113,-1384.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"602.614,-1384.84 599.862,-1374.61 595.633,-1384.32 602.614,-1384.84\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548348622832 -->\n",
       "<g class=\"node\" id=\"node16\"><title>140548348622832</title>\n",
       "<polygon fill=\"none\" points=\"898.5,-1328.5 898.5,-1374.5 1182.5,-1374.5 1182.5,-1328.5 898.5,-1328.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"961\" y=\"-1347.8\">dropout_3: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1023.5,-1328.5 1023.5,-1374.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1051\" y=\"-1359.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1023.5,-1351.5 1078.5,-1351.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1051\" y=\"-1336.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1078.5,-1328.5 1078.5,-1374.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-1359.3\">(None, 35, 256)</text>\n",
       "<polyline fill=\"none\" points=\"1078.5,-1351.5 1182.5,-1351.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-1336.3\">(None, 35, 256)</text>\n",
       "</g>\n",
       "<!-- 140548352894512&#45;&gt;140548348622832 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>140548352894512-&gt;140548348622832</title>\n",
       "<path d=\"M1049.95,-1411.37C1048.63,-1403.15 1047.11,-1393.66 1045.67,-1384.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1049.09,-1383.93 1044.05,-1374.61 1042.18,-1385.04 1049.09,-1383.93\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548347875056 -->\n",
       "<g class=\"node\" id=\"node17\"><title>140548347875056</title>\n",
       "<polygon fill=\"none\" points=\"397.5,-1245.5 397.5,-1291.5 809.5,-1291.5 809.5,-1245.5 397.5,-1245.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"527.5\" y=\"-1264.8\">batch_normalization_3: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"657.5,-1245.5 657.5,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"685\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"657.5,-1268.5 712.5,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"685\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"712.5,-1245.5 712.5,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"761\" y=\"-1276.3\">(None, 1, 256)</text>\n",
       "<polyline fill=\"none\" points=\"712.5,-1268.5 809.5,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"761\" y=\"-1253.3\">(None, 1, 256)</text>\n",
       "</g>\n",
       "<!-- 140548348030928&#45;&gt;140548347875056 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>140548348030928-&gt;140548347875056</title>\n",
       "<path d=\"M602.047,-1328.37C602.249,-1320.15 602.484,-1310.66 602.704,-1301.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"606.206,-1301.69 602.954,-1291.61 599.208,-1301.52 606.206,-1301.69\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548348624680 -->\n",
       "<g class=\"node\" id=\"node18\"><title>140548348624680</title>\n",
       "<polygon fill=\"none\" points=\"828,-1245.5 828,-1291.5 1247,-1291.5 1247,-1245.5 828,-1245.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-1264.8\">batch_normalization_2: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"1088,-1245.5 1088,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1115.5\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1088,-1268.5 1143,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1115.5\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1143,-1245.5 1143,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1195\" y=\"-1276.3\">(None, 35, 256)</text>\n",
       "<polyline fill=\"none\" points=\"1143,-1268.5 1247,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1195\" y=\"-1253.3\">(None, 35, 256)</text>\n",
       "</g>\n",
       "<!-- 140548348622832&#45;&gt;140548348624680 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>140548348622832-&gt;140548348624680</title>\n",
       "<path d=\"M1039.68,-1328.37C1039.38,-1320.15 1039.02,-1310.66 1038.69,-1301.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1042.19,-1301.47 1038.32,-1291.61 1035.19,-1301.73 1042.19,-1301.47\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548347323392 -->\n",
       "<g class=\"node\" id=\"node19\"><title>140548347323392</title>\n",
       "<polygon fill=\"none\" points=\"830,-1162.5 830,-1208.5 1227,-1208.5 1227,-1162.5 830,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"901\" y=\"-1181.8\">ca1: CrossAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"972,-1162.5 972,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"999.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"972,-1185.5 1027,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"999.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1027,-1162.5 1027,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1127\" y=\"-1193.3\">[(None, 1, 256), (None, 35, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"1027,-1185.5 1227,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1127\" y=\"-1170.3\">[(None, 35, 256), (1, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140548347875056&#45;&gt;140548347323392 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>140548347875056-&gt;140548347323392</title>\n",
       "<path d=\"M719.072,-1245.47C776,-1234.62 844.74,-1221.52 902.957,-1210.43\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"903.783,-1213.83 912.951,-1208.52 902.473,-1206.96 903.783,-1213.83\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548346754328 -->\n",
       "<g class=\"node\" id=\"node20\"><title>140548346754328</title>\n",
       "<polygon fill=\"none\" points=\"1245,-1162.5 1245,-1208.5 1642,-1208.5 1642,-1162.5 1245,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1316\" y=\"-1181.8\">ca2: CrossAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"1387,-1162.5 1387,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1414.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1387,-1185.5 1442,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1414.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1442,-1162.5 1442,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1542\" y=\"-1193.3\">[(None, 1, 256), (None, 35, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"1442,-1185.5 1642,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1542\" y=\"-1170.3\">[(None, 35, 256), (1, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140548347875056&#45;&gt;140548346754328 -->\n",
       "<g class=\"edge\" id=\"edge22\"><title>140548347875056-&gt;140548346754328</title>\n",
       "<path d=\"M809.57,-1245.95C812.904,-1245.63 816.216,-1245.31 819.5,-1245 1000.99,-1227.7 1049.11,-1227 1234.69,-1209.07\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1235.17,-1212.54 1244.79,-1208.09 1234.5,-1205.57 1235.17,-1212.54\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548345869536 -->\n",
       "<g class=\"node\" id=\"node21\"><title>140548345869536</title>\n",
       "<polygon fill=\"none\" points=\"0,-1162.5 0,-1208.5 397,-1208.5 397,-1162.5 0,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"71\" y=\"-1181.8\">ca3: CrossAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"142,-1162.5 142,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"142,-1185.5 197,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"197,-1162.5 197,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"297\" y=\"-1193.3\">[(None, 1, 256), (None, 35, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"197,-1185.5 397,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"297\" y=\"-1170.3\">[(None, 35, 256), (1, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140548347875056&#45;&gt;140548345869536 -->\n",
       "<g class=\"edge\" id=\"edge24\"><title>140548347875056-&gt;140548345869536</title>\n",
       "<path d=\"M493.367,-1245.47C439.23,-1234.65 373.883,-1221.58 318.479,-1210.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"319.103,-1207.05 308.611,-1208.52 317.731,-1213.92 319.103,-1207.05\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548344615880 -->\n",
       "<g class=\"node\" id=\"node22\"><title>140548344615880</title>\n",
       "<polygon fill=\"none\" points=\"415,-1162.5 415,-1208.5 812,-1208.5 812,-1162.5 415,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"486\" y=\"-1181.8\">ca4: CrossAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"557,-1162.5 557,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"584.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"557,-1185.5 612,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"584.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"612,-1162.5 612,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"712\" y=\"-1193.3\">[(None, 1, 256), (None, 35, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"612,-1185.5 812,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"712\" y=\"-1170.3\">[(None, 35, 256), (1, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140548347875056&#45;&gt;140548344615880 -->\n",
       "<g class=\"edge\" id=\"edge26\"><title>140548347875056-&gt;140548344615880</title>\n",
       "<path d=\"M606.233,-1245.37C607.247,-1237.15 608.419,-1227.66 609.522,-1218.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"613.019,-1218.96 610.771,-1208.61 606.072,-1218.1 613.019,-1218.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548348624680&#45;&gt;140548347323392 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>140548348624680-&gt;140548347323392</title>\n",
       "<path d=\"M1035.04,-1245.37C1034.13,-1237.15 1033.07,-1227.66 1032.08,-1218.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1035.54,-1218.16 1030.96,-1208.61 1028.58,-1218.93 1035.54,-1218.16\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548348624680&#45;&gt;140548346754328 -->\n",
       "<g class=\"edge\" id=\"edge23\"><title>140548348624680-&gt;140548346754328</title>\n",
       "<path d=\"M1147.91,-1245.47C1202.18,-1234.65 1267.68,-1221.58 1323.23,-1210.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1324,-1213.91 1333.12,-1208.52 1322.63,-1207.05 1324,-1213.91\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548348624680&#45;&gt;140548345869536 -->\n",
       "<g class=\"edge\" id=\"edge25\"><title>140548348624680-&gt;140548345869536</title>\n",
       "<path d=\"M827.988,-1245.91C824.803,-1245.6 821.639,-1245.3 818.5,-1245 638.298,-1227.89 590.541,-1226.98 407.057,-1209.14\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"407.369,-1205.66 397.076,-1208.17 406.689,-1212.62 407.369,-1205.66\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548348624680&#45;&gt;140548344615880 -->\n",
       "<g class=\"edge\" id=\"edge27\"><title>140548348624680-&gt;140548344615880</title>\n",
       "<path d=\"M922.2,-1245.47C865.406,-1234.62 796.828,-1221.52 738.747,-1210.43\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"739.256,-1206.96 728.777,-1208.52 737.942,-1213.84 739.256,-1206.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548343886624 -->\n",
       "<g class=\"node\" id=\"node23\"><title>140548343886624</title>\n",
       "<polygon fill=\"none\" points=\"511,-1079.5 511,-1125.5 1130,-1125.5 1130,-1079.5 511,-1079.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"595\" y=\"-1098.8\">concatenate_2: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"679,-1079.5 679,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"706.5\" y=\"-1110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"679,-1102.5 734,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"706.5\" y=\"-1087.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"734,-1079.5 734,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"932\" y=\"-1110.3\">[(None, 35, 256), (None, 35, 256), (None, 35, 256), (None, 35, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"734,-1102.5 1130,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"932\" y=\"-1087.3\">(None, 35, 1024)</text>\n",
       "</g>\n",
       "<!-- 140548347323392&#45;&gt;140548343886624 -->\n",
       "<g class=\"edge\" id=\"edge28\"><title>140548347323392-&gt;140548343886624</title>\n",
       "<path d=\"M971.938,-1162.47C945.573,-1152.21 914.032,-1139.92 886.577,-1129.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"887.639,-1125.89 877.051,-1125.52 885.099,-1132.41 887.639,-1125.89\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548346754328&#45;&gt;140548343886624 -->\n",
       "<g class=\"edge\" id=\"edge29\"><title>140548346754328-&gt;140548343886624</title>\n",
       "<path d=\"M1274.09,-1162.47C1189.17,-1151.43 1086.33,-1138.06 1000.05,-1126.84\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1000.25,-1123.34 989.881,-1125.52 999.346,-1130.28 1000.25,-1123.34\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548345869536&#45;&gt;140548343886624 -->\n",
       "<g class=\"edge\" id=\"edge30\"><title>140548345869536-&gt;140548343886624</title>\n",
       "<path d=\"M367.643,-1162.47C452.422,-1151.43 555.101,-1138.06 641.239,-1126.84\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"641.927,-1130.28 651.391,-1125.52 641.023,-1123.34 641.927,-1130.28\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548344615880&#45;&gt;140548343886624 -->\n",
       "<g class=\"edge\" id=\"edge31\"><title>140548344615880-&gt;140548343886624</title>\n",
       "<path d=\"M669.79,-1162.47C696.028,-1152.21 727.418,-1139.92 754.741,-1129.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"756.184,-1132.43 764.221,-1125.52 753.633,-1125.91 756.184,-1132.43\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548343525328 -->\n",
       "<g class=\"node\" id=\"node24\"><title>140548343525328</title>\n",
       "<polygon fill=\"none\" points=\"675.5,-996.5 675.5,-1042.5 965.5,-1042.5 965.5,-996.5 675.5,-996.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"738\" y=\"-1015.8\">dropout_4: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"800.5,-996.5 800.5,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"828\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"800.5,-1019.5 855.5,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"828\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"855.5,-996.5 855.5,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910.5\" y=\"-1027.3\">(None, 35, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"855.5,-1019.5 965.5,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910.5\" y=\"-1004.3\">(None, 35, 1024)</text>\n",
       "</g>\n",
       "<!-- 140548343886624&#45;&gt;140548343525328 -->\n",
       "<g class=\"edge\" id=\"edge32\"><title>140548343886624-&gt;140548343525328</title>\n",
       "<path d=\"M820.5,-1079.37C820.5,-1071.15 820.5,-1061.66 820.5,-1052.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-1052.61 820.5,-1042.61 817,-1052.61 824,-1052.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548347446776 -->\n",
       "<g class=\"node\" id=\"node25\"><title>140548347446776</title>\n",
       "<polygon fill=\"none\" points=\"608,-913.5 608,-959.5 1033,-959.5 1033,-913.5 608,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"738\" y=\"-932.8\">batch_normalization_4: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"868,-913.5 868,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"895.5\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"868,-936.5 923,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"895.5\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"923,-913.5 923,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"978\" y=\"-944.3\">(None, 35, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"923,-936.5 1033,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"978\" y=\"-921.3\">(None, 35, 1024)</text>\n",
       "</g>\n",
       "<!-- 140548343525328&#45;&gt;140548347446776 -->\n",
       "<g class=\"edge\" id=\"edge33\"><title>140548343525328-&gt;140548347446776</title>\n",
       "<path d=\"M820.5,-996.366C820.5,-988.152 820.5,-978.658 820.5,-969.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-969.607 820.5,-959.607 817,-969.607 824,-969.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548347445936 -->\n",
       "<g class=\"node\" id=\"node26\"><title>140548347445936</title>\n",
       "<polygon fill=\"none\" points=\"676.5,-830.5 676.5,-876.5 964.5,-876.5 964.5,-830.5 676.5,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"738\" y=\"-849.8\">conv1d_4: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"799.5,-830.5 799.5,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"827\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"799.5,-853.5 854.5,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"827\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"854.5,-830.5 854.5,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"909.5\" y=\"-861.3\">(None, 35, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"854.5,-853.5 964.5,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"909.5\" y=\"-838.3\">(None, 18, 256)</text>\n",
       "</g>\n",
       "<!-- 140548347446776&#45;&gt;140548347445936 -->\n",
       "<g class=\"edge\" id=\"edge34\"><title>140548347446776-&gt;140548347445936</title>\n",
       "<path d=\"M820.5,-913.366C820.5,-905.152 820.5,-895.658 820.5,-886.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-886.607 820.5,-876.607 817,-886.607 824,-886.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548342808360 -->\n",
       "<g class=\"node\" id=\"node27\"><title>140548342808360</title>\n",
       "<polygon fill=\"none\" points=\"679.5,-747.5 679.5,-793.5 961.5,-793.5 961.5,-747.5 679.5,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"741\" y=\"-766.8\">conv1d_5: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"802.5,-747.5 802.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"830\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"802.5,-770.5 857.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"830\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"857.5,-747.5 857.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"909.5\" y=\"-778.3\">(None, 18, 256)</text>\n",
       "<polyline fill=\"none\" points=\"857.5,-770.5 961.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"909.5\" y=\"-755.3\">(None, 9, 256)</text>\n",
       "</g>\n",
       "<!-- 140548347445936&#45;&gt;140548342808360 -->\n",
       "<g class=\"edge\" id=\"edge35\"><title>140548347445936-&gt;140548342808360</title>\n",
       "<path d=\"M820.5,-830.366C820.5,-822.152 820.5,-812.658 820.5,-803.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-803.607 820.5,-793.607 817,-803.607 824,-803.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548341363544 -->\n",
       "<g class=\"node\" id=\"node28\"><title>140548341363544</title>\n",
       "<polygon fill=\"none\" points=\"683,-664.5 683,-710.5 958,-710.5 958,-664.5 683,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"744.5\" y=\"-683.8\">conv1d_6: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"806,-664.5 806,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"833.5\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"806,-687.5 861,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"833.5\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"861,-664.5 861,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"909.5\" y=\"-695.3\">(None, 9, 256)</text>\n",
       "<polyline fill=\"none\" points=\"861,-687.5 958,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"909.5\" y=\"-672.3\">(None, 5, 256)</text>\n",
       "</g>\n",
       "<!-- 140548342808360&#45;&gt;140548341363544 -->\n",
       "<g class=\"edge\" id=\"edge36\"><title>140548342808360-&gt;140548341363544</title>\n",
       "<path d=\"M820.5,-747.366C820.5,-739.152 820.5,-729.658 820.5,-720.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-720.607 820.5,-710.607 817,-720.607 824,-720.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548340965160 -->\n",
       "<g class=\"node\" id=\"node29\"><title>140548340965160</title>\n",
       "<polygon fill=\"none\" points=\"683,-581.5 683,-627.5 958,-627.5 958,-581.5 683,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"744.5\" y=\"-600.8\">conv1d_7: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"806,-581.5 806,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"833.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"806,-604.5 861,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"833.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"861,-581.5 861,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"909.5\" y=\"-612.3\">(None, 5, 256)</text>\n",
       "<polyline fill=\"none\" points=\"861,-604.5 958,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"909.5\" y=\"-589.3\">(None, 3, 256)</text>\n",
       "</g>\n",
       "<!-- 140548341363544&#45;&gt;140548340965160 -->\n",
       "<g class=\"edge\" id=\"edge37\"><title>140548341363544-&gt;140548340965160</title>\n",
       "<path d=\"M820.5,-664.366C820.5,-656.152 820.5,-646.658 820.5,-637.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-637.607 820.5,-627.607 817,-637.607 824,-637.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548341156048 -->\n",
       "<g class=\"node\" id=\"node30\"><title>140548341156048</title>\n",
       "<polygon fill=\"none\" points=\"682,-498.5 682,-544.5 959,-544.5 959,-498.5 682,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"744.5\" y=\"-517.8\">dropout_5: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"807,-498.5 807,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"834.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"807,-521.5 862,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"834.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"862,-498.5 862,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910.5\" y=\"-529.3\">(None, 3, 256)</text>\n",
       "<polyline fill=\"none\" points=\"862,-521.5 959,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910.5\" y=\"-506.3\">(None, 3, 256)</text>\n",
       "</g>\n",
       "<!-- 140548340965160&#45;&gt;140548341156048 -->\n",
       "<g class=\"edge\" id=\"edge38\"><title>140548340965160-&gt;140548341156048</title>\n",
       "<path d=\"M820.5,-581.366C820.5,-573.152 820.5,-563.658 820.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-554.607 820.5,-544.607 817,-554.607 824,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548341156104 -->\n",
       "<g class=\"node\" id=\"node31\"><title>140548341156104</title>\n",
       "<polygon fill=\"none\" points=\"614.5,-415.5 614.5,-461.5 1026.5,-461.5 1026.5,-415.5 614.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"744.5\" y=\"-434.8\">batch_normalization_5: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"874.5,-415.5 874.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"902\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"874.5,-438.5 929.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"902\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"929.5,-415.5 929.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"978\" y=\"-446.3\">(None, 3, 256)</text>\n",
       "<polyline fill=\"none\" points=\"929.5,-438.5 1026.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"978\" y=\"-423.3\">(None, 3, 256)</text>\n",
       "</g>\n",
       "<!-- 140548341156048&#45;&gt;140548341156104 -->\n",
       "<g class=\"edge\" id=\"edge39\"><title>140548341156048-&gt;140548341156104</title>\n",
       "<path d=\"M820.5,-498.366C820.5,-490.152 820.5,-480.658 820.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-471.607 820.5,-461.607 817,-471.607 824,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548341156776 -->\n",
       "<g class=\"node\" id=\"node32\"><title>140548341156776</title>\n",
       "<polygon fill=\"none\" points=\"583,-332.5 583,-378.5 1058,-378.5 1058,-332.5 583,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"744.5\" y=\"-351.8\">global_average_pooling1d_1: GlobalAveragePooling1D</text>\n",
       "<polyline fill=\"none\" points=\"906,-332.5 906,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"933.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"906,-355.5 961,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"933.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"961,-332.5 961,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1009.5\" y=\"-363.3\">(None, 3, 256)</text>\n",
       "<polyline fill=\"none\" points=\"961,-355.5 1058,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1009.5\" y=\"-340.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140548341156104&#45;&gt;140548341156776 -->\n",
       "<g class=\"edge\" id=\"edge40\"><title>140548341156104-&gt;140548341156776</title>\n",
       "<path d=\"M820.5,-415.366C820.5,-407.152 820.5,-397.658 820.5,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-388.607 820.5,-378.607 817,-388.607 824,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548340772648 -->\n",
       "<g class=\"node\" id=\"node33\"><title>140548340772648</title>\n",
       "<polygon fill=\"none\" points=\"700.5,-249.5 700.5,-295.5 940.5,-295.5 940.5,-249.5 700.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"751.5\" y=\"-268.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"802.5,-249.5 802.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"830\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"802.5,-272.5 857.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"830\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"857.5,-249.5 857.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"899\" y=\"-280.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"857.5,-272.5 940.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"899\" y=\"-257.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140548341156776&#45;&gt;140548340772648 -->\n",
       "<g class=\"edge\" id=\"edge41\"><title>140548341156776-&gt;140548340772648</title>\n",
       "<path d=\"M820.5,-332.366C820.5,-324.152 820.5,-314.658 820.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-305.607 820.5,-295.607 817,-305.607 824,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548340364568 -->\n",
       "<g class=\"node\" id=\"node34\"><title>140548340364568</title>\n",
       "<polygon fill=\"none\" points=\"689,-166.5 689,-212.5 952,-212.5 952,-166.5 689,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"751.5\" y=\"-185.8\">dropout_6: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"814,-166.5 814,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"841.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"814,-189.5 869,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"841.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"869,-166.5 869,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910.5\" y=\"-197.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"869,-189.5 952,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910.5\" y=\"-174.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140548340772648&#45;&gt;140548340364568 -->\n",
       "<g class=\"edge\" id=\"edge42\"><title>140548340772648-&gt;140548340364568</title>\n",
       "<path d=\"M820.5,-249.366C820.5,-241.152 820.5,-231.658 820.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-222.607 820.5,-212.607 817,-222.607 824,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548340364848 -->\n",
       "<g class=\"node\" id=\"node35\"><title>140548340364848</title>\n",
       "<polygon fill=\"none\" points=\"621.5,-83.5 621.5,-129.5 1019.5,-129.5 1019.5,-83.5 621.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"751.5\" y=\"-102.8\">batch_normalization_6: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"881.5,-83.5 881.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"909\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"881.5,-106.5 936.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"909\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"936.5,-83.5 936.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"978\" y=\"-114.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"936.5,-106.5 1019.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"978\" y=\"-91.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140548340364568&#45;&gt;140548340364848 -->\n",
       "<g class=\"edge\" id=\"edge43\"><title>140548340364568-&gt;140548340364848</title>\n",
       "<path d=\"M820.5,-166.366C820.5,-158.152 820.5,-148.658 820.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-139.607 820.5,-129.607 817,-139.607 824,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140548340068984 -->\n",
       "<g class=\"node\" id=\"node36\"><title>140548340068984</title>\n",
       "<polygon fill=\"none\" points=\"659,-0.5 659,-46.5 982,-46.5 982,-0.5 659,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"751.5\" y=\"-19.8\">output_headline_vector: Dense</text>\n",
       "<polyline fill=\"none\" points=\"844,-0.5 844,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"871.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"844,-23.5 899,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"871.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"899,-0.5 899,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"940.5\" y=\"-31.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"899,-23.5 982,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"940.5\" y=\"-8.3\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 140548340364848&#45;&gt;140548340068984 -->\n",
       "<g class=\"edge\" id=\"edge44\"><title>140548340364848-&gt;140548340068984</title>\n",
       "<path d=\"M820.5,-83.3664C820.5,-75.1516 820.5,-65.6579 820.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"824,-56.6068 820.5,-46.6068 817,-56.6069 824,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    inp_sentence_vectors = Input(shape=(max_sentences, 300), name='sentence_vectors')\n",
    "    inp_headline_vector = Input(shape=(300,), name='input_headline_vector')\n",
    "    conv1 = Conv1D(filters=16,kernel_size=3,strides=1,activation='relu', padding='same')(inp_sentence_vectors)\n",
    "    conv1 = Dropout(0.5)(conv1)\n",
    "    conv2 = Conv1D(filters=32,kernel_size=3,strides=1,activation='relu', padding='same')(conv1)\n",
    "    conv2 = Dropout(0.5)(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    sent_sa_feat_1, sent_beta_1, sent_gamma_1 = SelfAttention(int(conv2.shape[-1]), name = 'sa1')(conv2)\n",
    "    sent_sa_feat_2, sent_beta_2, sent_gamma_2 = SelfAttention(int(conv2.shape[-1]), name = 'sa2')(conv2)\n",
    "    sent_sa_feat_3, sent_beta_3, sent_gamma_3 = SelfAttention(int(conv2.shape[-1]), name = 'sa3')(conv2)\n",
    "    sent_sa_feat_4, sent_beta_4, sent_gamma_4 = SelfAttention(int(conv2.shape[-1]), name = 'sa4')(conv2)\n",
    "    concat1 = Concatenate()([sent_sa_feat_1,sent_sa_feat_2,sent_sa_feat_3,sent_sa_feat_4])\n",
    "    conv3 = Conv1D(filters=256,kernel_size=3, strides=1, activation='relu', padding='same')(concat1)\n",
    "    conv3 = Dropout(0.5)(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    headline = Dense(256, activation='relu')(inp_headline_vector)\n",
    "    headline = Lambda(lambda x:K.expand_dims(x, axis=1))(headline)\n",
    "    headline = BatchNormalization()(headline)\n",
    "    sent_hd_sa_feat_1, sent_hd_beta_1, sent_hd_gamma_1 = CrossAttention(int(conv3.shape[-1]), name = 'ca1')([headline,conv3])\n",
    "    sent_hd_sa_feat_2, sent_hd_beta_2, sent_hd_gamma_2 = CrossAttention(int(conv3.shape[-1]), name = 'ca2')([headline,conv3])\n",
    "    sent_hd_sa_feat_3, sent_hd_beta_3, sent_hd_gamma_3 = CrossAttention(int(conv3.shape[-1]), name = 'ca3')([headline,conv3])\n",
    "    sent_hd_sa_feat_4, sent_hd_beta_4, sent_hd_gamma_4 = CrossAttention(int(conv3.shape[-1]), name = 'ca4')([headline,conv3])  \n",
    "    concat3 = Concatenate()([sent_hd_sa_feat_1,sent_hd_sa_feat_2,sent_hd_sa_feat_3,sent_hd_sa_feat_4])\n",
    "    concat3 = Dropout(0.5)(concat3)\n",
    "    concat3 = BatchNormalization()(concat3)\n",
    "    conv5 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(concat3)\n",
    "    conv6 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv5)\n",
    "    conv7 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv6)\n",
    "    conv8 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv7)\n",
    "    conv8 = Dropout(0.5)(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    gap = GlobalAveragePooling1D()(conv8)\n",
    "#     repeat = RepeatVector(50)(gap)\n",
    "#     lstm = LSTM(256,return_sequences=True)(repeat)\n",
    "    dense1 = Dense(512,activation='relu')(gap)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    gen_hd_vector = Dense(300,activation='linear', name='output_headline_vector')(dense1)\n",
    "    model = Model([inp_sentence_vectors,inp_headline_vector],gen_hd_vector)\n",
    "    return model\n",
    "model = build_model()\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.0001,beta_1=0.0,beta_2=0.99),loss='mse')\n",
    "model.summary()\n",
    "# print('model params:',model.count_params())\n",
    "SVG(model_to_dot(model,show_layer_names=True,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now()\n",
    "mc = ModelCheckpoint('weights/dnf300_sa_sent_hd_vector_gl.hdf5',save_best_only=True,save_weights_only=True)\n",
    "tb = TensorBoard(batch_size=32,log_dir='logs/dnf300_sa_sent_hd_vector_gl/{0}'.format(dt.timestamp()),write_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.0335 - val_loss: 0.0362\n",
      "Epoch 2/2000\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0318 - val_loss: 0.0349\n",
      "Epoch 3/2000\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0334 - val_loss: 0.0349\n",
      "Epoch 4/2000\n",
      "4/4 [==============================] - 1s 224ms/step - loss: 0.0331 - val_loss: 0.0346\n",
      "Epoch 5/2000\n",
      "4/4 [==============================] - 2s 439ms/step - loss: 0.0326 - val_loss: 0.0321\n",
      "Epoch 6/2000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 0.0317 - val_loss: 0.0341\n",
      "Epoch 7/2000\n",
      "4/4 [==============================] - 2s 389ms/step - loss: 0.0314 - val_loss: 0.0347\n",
      "Epoch 8/2000\n",
      "4/4 [==============================] - 2s 409ms/step - loss: 0.0300 - val_loss: 0.0300\n",
      "Epoch 9/2000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 0.0302 - val_loss: 0.0299\n",
      "Epoch 10/2000\n",
      "4/4 [==============================] - 2s 407ms/step - loss: 0.0303 - val_loss: 0.0293\n",
      "Epoch 11/2000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 0.0297 - val_loss: 0.0321\n",
      "Epoch 12/2000\n",
      "4/4 [==============================] - 2s 385ms/step - loss: 0.0309 - val_loss: 0.0315\n",
      "Epoch 13/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0297 - val_loss: 0.0291\n",
      "Epoch 14/2000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 0.0297 - val_loss: 0.0274\n",
      "Epoch 15/2000\n",
      "4/4 [==============================] - 2s 425ms/step - loss: 0.0277 - val_loss: 0.0295\n",
      "Epoch 16/2000\n",
      "4/4 [==============================] - 2s 404ms/step - loss: 0.0273 - val_loss: 0.0273\n",
      "Epoch 17/2000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 0.0273 - val_loss: 0.0272\n",
      "Epoch 18/2000\n",
      "4/4 [==============================] - 2s 407ms/step - loss: 0.0270 - val_loss: 0.0292\n",
      "Epoch 19/2000\n",
      "4/4 [==============================] - 2s 410ms/step - loss: 0.0266 - val_loss: 0.0266\n",
      "Epoch 20/2000\n",
      "4/4 [==============================] - 2s 382ms/step - loss: 0.0266 - val_loss: 0.0260\n",
      "Epoch 21/2000\n",
      "4/4 [==============================] - 2s 414ms/step - loss: 0.0254 - val_loss: 0.0255\n",
      "Epoch 22/2000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 0.0253 - val_loss: 0.0232\n",
      "Epoch 23/2000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 0.0247 - val_loss: 0.0244\n",
      "Epoch 24/2000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 0.0247 - val_loss: 0.0262\n",
      "Epoch 25/2000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 0.0236 - val_loss: 0.0265\n",
      "Epoch 26/2000\n",
      "4/4 [==============================] - 2s 410ms/step - loss: 0.0231 - val_loss: 0.0242\n",
      "Epoch 27/2000\n",
      "4/4 [==============================] - 2s 409ms/step - loss: 0.0227 - val_loss: 0.0229\n",
      "Epoch 28/2000\n",
      "4/4 [==============================] - 2s 385ms/step - loss: 0.0233 - val_loss: 0.0255\n",
      "Epoch 29/2000\n",
      "4/4 [==============================] - 2s 410ms/step - loss: 0.0228 - val_loss: 0.0226\n",
      "Epoch 30/2000\n",
      "4/4 [==============================] - 2s 407ms/step - loss: 0.0228 - val_loss: 0.0233\n",
      "Epoch 31/2000\n",
      "4/4 [==============================] - 2s 417ms/step - loss: 0.0224 - val_loss: 0.0212\n",
      "Epoch 32/2000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 0.0218 - val_loss: 0.0218\n",
      "Epoch 33/2000\n",
      "4/4 [==============================] - 2s 419ms/step - loss: 0.0224 - val_loss: 0.0234\n",
      "Epoch 34/2000\n",
      "4/4 [==============================] - 2s 410ms/step - loss: 0.0219 - val_loss: 0.0228\n",
      "Epoch 35/2000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 0.0211 - val_loss: 0.0228\n",
      "Epoch 36/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0207 - val_loss: 0.0230\n",
      "Epoch 37/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0205 - val_loss: 0.0211\n",
      "Epoch 38/2000\n",
      "4/4 [==============================] - 2s 402ms/step - loss: 0.0202 - val_loss: 0.0211\n",
      "Epoch 39/2000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 0.0204 - val_loss: 0.0201\n",
      "Epoch 40/2000\n",
      "4/4 [==============================] - 2s 409ms/step - loss: 0.0209 - val_loss: 0.0188\n",
      "Epoch 41/2000\n",
      "4/4 [==============================] - 2s 385ms/step - loss: 0.0201 - val_loss: 0.0214\n",
      "Epoch 42/2000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 0.0196 - val_loss: 0.0197\n",
      "Epoch 43/2000\n",
      "4/4 [==============================] - 2s 406ms/step - loss: 0.0189 - val_loss: 0.0214\n",
      "Epoch 44/2000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 0.0194 - val_loss: 0.0202\n",
      "Epoch 45/2000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0198 - val_loss: 0.0200\n",
      "Epoch 46/2000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 0.0185 - val_loss: 0.0197\n",
      "Epoch 47/2000\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 0.0190 - val_loss: 0.0187\n",
      "Epoch 48/2000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 0.0187 - val_loss: 0.0186\n",
      "Epoch 49/2000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 0.0178 - val_loss: 0.0184\n",
      "Epoch 50/2000\n",
      "4/4 [==============================] - 2s 384ms/step - loss: 0.0179 - val_loss: 0.0195\n",
      "Epoch 51/2000\n",
      "4/4 [==============================] - 2s 414ms/step - loss: 0.0171 - val_loss: 0.0179\n",
      "Epoch 52/2000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 0.0170 - val_loss: 0.0174\n",
      "Epoch 53/2000\n",
      "4/4 [==============================] - 2s 400ms/step - loss: 0.0161 - val_loss: 0.0170\n",
      "Epoch 54/2000\n",
      "4/4 [==============================] - 2s 402ms/step - loss: 0.0178 - val_loss: 0.0178\n",
      "Epoch 55/2000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 0.0162 - val_loss: 0.0202\n",
      "Epoch 56/2000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0165 - val_loss: 0.0173\n",
      "Epoch 57/2000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 0.0157 - val_loss: 0.0167\n",
      "Epoch 58/2000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 0.0165 - val_loss: 0.0170\n",
      "Epoch 59/2000\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 0.0156 - val_loss: 0.0162\n",
      "Epoch 60/2000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 0.0154 - val_loss: 0.0194\n",
      "Epoch 61/2000\n",
      "4/4 [==============================] - 2s 409ms/step - loss: 0.0148 - val_loss: 0.0168\n",
      "Epoch 62/2000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0144 - val_loss: 0.0153\n",
      "Epoch 63/2000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 0.0150 - val_loss: 0.0156\n",
      "Epoch 64/2000\n",
      "4/4 [==============================] - 2s 418ms/step - loss: 0.0149 - val_loss: 0.0175\n",
      "Epoch 65/2000\n",
      "4/4 [==============================] - 2s 405ms/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 66/2000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 0.0141 - val_loss: 0.0158\n",
      "Epoch 67/2000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 0.0153 - val_loss: 0.0150\n",
      "Epoch 68/2000\n",
      "4/4 [==============================] - 2s 385ms/step - loss: 0.0139 - val_loss: 0.0137\n",
      "Epoch 69/2000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 0.0146 - val_loss: 0.0153\n",
      "Epoch 70/2000\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 0.0136 - val_loss: 0.0145\n",
      "Epoch 71/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0132 - val_loss: 0.0134\n",
      "Epoch 72/2000\n",
      "4/4 [==============================] - 2s 407ms/step - loss: 0.0141 - val_loss: 0.0148\n",
      "Epoch 73/2000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 0.0137 - val_loss: 0.0146\n",
      "Epoch 74/2000\n",
      "4/4 [==============================] - 2s 421ms/step - loss: 0.0147 - val_loss: 0.0149\n",
      "Epoch 75/2000\n",
      "4/4 [==============================] - 2s 448ms/step - loss: 0.0143 - val_loss: 0.0129\n",
      "Epoch 76/2000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 0.0133 - val_loss: 0.0146\n",
      "Epoch 77/2000\n",
      "4/4 [==============================] - 2s 465ms/step - loss: 0.0135 - val_loss: 0.0142\n",
      "Epoch 78/2000\n",
      "4/4 [==============================] - 2s 490ms/step - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 79/2000\n",
      "4/4 [==============================] - 2s 454ms/step - loss: 0.0137 - val_loss: 0.0121\n",
      "Epoch 80/2000\n",
      "4/4 [==============================] - 2s 441ms/step - loss: 0.0128 - val_loss: 0.0138\n",
      "Epoch 81/2000\n",
      "4/4 [==============================] - 2s 427ms/step - loss: 0.0130 - val_loss: 0.0134\n",
      "Epoch 82/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 437ms/step - loss: 0.0130 - val_loss: 0.0162\n",
      "Epoch 83/2000\n",
      "4/4 [==============================] - 2s 424ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 84/2000\n",
      "4/4 [==============================] - 2s 463ms/step - loss: 0.0122 - val_loss: 0.0140\n",
      "Epoch 85/2000\n",
      "4/4 [==============================] - 2s 460ms/step - loss: 0.0127 - val_loss: 0.0121\n",
      "Epoch 86/2000\n",
      "4/4 [==============================] - 2s 451ms/step - loss: 0.0127 - val_loss: 0.0129\n",
      "Epoch 87/2000\n",
      "4/4 [==============================] - 2s 425ms/step - loss: 0.0127 - val_loss: 0.0121\n",
      "Epoch 88/2000\n",
      "4/4 [==============================] - 2s 414ms/step - loss: 0.0123 - val_loss: 0.0140\n",
      "Epoch 89/2000\n",
      "4/4 [==============================] - 2s 456ms/step - loss: 0.0122 - val_loss: 0.0120\n",
      "Epoch 90/2000\n",
      "4/4 [==============================] - 2s 456ms/step - loss: 0.0129 - val_loss: 0.0127\n",
      "Epoch 91/2000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 0.0119 - val_loss: 0.0125\n",
      "Epoch 92/2000\n",
      "4/4 [==============================] - 2s 429ms/step - loss: 0.0122 - val_loss: 0.0113\n",
      "Epoch 93/2000\n",
      "4/4 [==============================] - 2s 486ms/step - loss: 0.0112 - val_loss: 0.0121\n",
      "Epoch 94/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0110 - val_loss: 0.0126\n",
      "Epoch 95/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0119 - val_loss: 0.0116\n",
      "Epoch 96/2000\n",
      "4/4 [==============================] - 4s 909ms/step - loss: 0.0127 - val_loss: 0.0118\n",
      "Epoch 97/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 98/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0120 - val_loss: 0.0110\n",
      "Epoch 99/2000\n",
      "4/4 [==============================] - 3s 754ms/step - loss: 0.0113 - val_loss: 0.0119\n",
      "Epoch 100/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.0117 - val_loss: 0.0119\n",
      "Epoch 101/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 102/2000\n",
      "4/4 [==============================] - 2s 610ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 103/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0115 - val_loss: 0.0120\n",
      "Epoch 104/2000\n",
      "4/4 [==============================] - 2s 616ms/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 105/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 106/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 107/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0112 - val_loss: 0.0121\n",
      "Epoch 108/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 109/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 110/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 111/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 112/2000\n",
      "4/4 [==============================] - 3s 795ms/step - loss: 0.0122 - val_loss: 0.0099\n",
      "Epoch 113/2000\n",
      "4/4 [==============================] - 2s 542ms/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 114/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0109 - val_loss: 0.0125\n",
      "Epoch 115/2000\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 116/2000\n",
      "4/4 [==============================] - 3s 846ms/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 117/2000\n",
      "4/4 [==============================] - 3s 762ms/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 118/2000\n",
      "4/4 [==============================] - 3s 818ms/step - loss: 0.0112 - val_loss: 0.0127\n",
      "Epoch 119/2000\n",
      "4/4 [==============================] - 3s 810ms/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 120/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.0119 - val_loss: 0.0117\n",
      "Epoch 121/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 122/2000\n",
      "4/4 [==============================] - 4s 961ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 123/2000\n",
      "4/4 [==============================] - 3s 790ms/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 124/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 125/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.0112 - val_loss: 0.0118\n",
      "Epoch 126/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 127/2000\n",
      "4/4 [==============================] - 3s 827ms/step - loss: 0.0099 - val_loss: 0.0114\n",
      "Epoch 128/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.0108 - val_loss: 0.0119\n",
      "Epoch 129/2000\n",
      "4/4 [==============================] - 2s 584ms/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 130/2000\n",
      "4/4 [==============================] - 3s 784ms/step - loss: 0.0104 - val_loss: 0.0123\n",
      "Epoch 131/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 132/2000\n",
      "4/4 [==============================] - 2s 623ms/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 133/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0105 - val_loss: 0.0130\n",
      "Epoch 134/2000\n",
      "4/4 [==============================] - 3s 737ms/step - loss: 0.0098 - val_loss: 0.0122\n",
      "Epoch 135/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0100 - val_loss: 0.0094\n",
      "Epoch 136/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 137/2000\n",
      "4/4 [==============================] - 2s 589ms/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 138/2000\n",
      "4/4 [==============================] - 3s 649ms/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 139/2000\n",
      "4/4 [==============================] - 3s 807ms/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 140/2000\n",
      "4/4 [==============================] - 2s 599ms/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 141/2000\n",
      "4/4 [==============================] - 2s 593ms/step - loss: 0.0099 - val_loss: 0.0110\n",
      "Epoch 142/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 143/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 144/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 145/2000\n",
      "4/4 [==============================] - 2s 609ms/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 146/2000\n",
      "4/4 [==============================] - 3s 649ms/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 147/2000\n",
      "4/4 [==============================] - 3s 723ms/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 148/2000\n",
      "4/4 [==============================] - 3s 646ms/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 149/2000\n",
      "4/4 [==============================] - 3s 749ms/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 150/2000\n",
      "4/4 [==============================] - 3s 715ms/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 151/2000\n",
      "4/4 [==============================] - 3s 798ms/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 152/2000\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 153/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 154/2000\n",
      "4/4 [==============================] - 2s 611ms/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 155/2000\n",
      "4/4 [==============================] - 3s 776ms/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 156/2000\n",
      "4/4 [==============================] - 3s 776ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 157/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 158/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 159/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 160/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 161/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 162/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 772ms/step - loss: 0.0105 - val_loss: 0.0131\n",
      "Epoch 163/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 164/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 165/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 166/2000\n",
      "4/4 [==============================] - 3s 738ms/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 167/2000\n",
      "4/4 [==============================] - 2s 619ms/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 168/2000\n",
      "4/4 [==============================] - 2s 609ms/step - loss: 0.0106 - val_loss: 0.0134\n",
      "Epoch 169/2000\n",
      "4/4 [==============================] - 2s 557ms/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 170/2000\n",
      "4/4 [==============================] - 2s 564ms/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 171/2000\n",
      "4/4 [==============================] - 2s 536ms/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 172/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 173/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0106 - val_loss: 0.0087\n",
      "Epoch 174/2000\n",
      "4/4 [==============================] - 3s 638ms/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 175/2000\n",
      "4/4 [==============================] - 2s 621ms/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 176/2000\n",
      "4/4 [==============================] - 3s 861ms/step - loss: 0.0100 - val_loss: 0.0115\n",
      "Epoch 177/2000\n",
      "4/4 [==============================] - 3s 747ms/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 178/2000\n",
      "4/4 [==============================] - 3s 775ms/step - loss: 0.0109 - val_loss: 0.0117\n",
      "Epoch 179/2000\n",
      "4/4 [==============================] - 3s 829ms/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 180/2000\n",
      "4/4 [==============================] - 3s 791ms/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 181/2000\n",
      "4/4 [==============================] - 3s 847ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 182/2000\n",
      "4/4 [==============================] - 3s 850ms/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 183/2000\n",
      "4/4 [==============================] - 3s 837ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 184/2000\n",
      "4/4 [==============================] - 3s 724ms/step - loss: 0.0111 - val_loss: 0.0095\n",
      "Epoch 185/2000\n",
      "4/4 [==============================] - 4s 879ms/step - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 186/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 187/2000\n",
      "4/4 [==============================] - 3s 795ms/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 188/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 0.0113 - val_loss: 0.0103\n",
      "Epoch 189/2000\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 190/2000\n",
      "4/4 [==============================] - 2s 564ms/step - loss: 0.0102 - val_loss: 0.0113\n",
      "Epoch 191/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 192/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0104 - val_loss: 0.0115\n",
      "Epoch 193/2000\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 194/2000\n",
      "4/4 [==============================] - 2s 623ms/step - loss: 0.0100 - val_loss: 0.0125\n",
      "Epoch 195/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 196/2000\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.0107 - val_loss: 0.0093\n",
      "Epoch 197/2000\n",
      "4/4 [==============================] - 3s 778ms/step - loss: 0.0120 - val_loss: 0.0117\n",
      "Epoch 198/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 199/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.0096 - val_loss: 0.0114\n",
      "Epoch 200/2000\n",
      "4/4 [==============================] - 3s 735ms/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 201/2000\n",
      "4/4 [==============================] - 3s 809ms/step - loss: 0.0107 - val_loss: 0.0098\n",
      "Epoch 202/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 203/2000\n",
      "4/4 [==============================] - 2s 593ms/step - loss: 0.0116 - val_loss: 0.0108\n",
      "Epoch 204/2000\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 205/2000\n",
      "4/4 [==============================] - 2s 608ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 206/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 207/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 208/2000\n",
      "4/4 [==============================] - 3s 753ms/step - loss: 0.0103 - val_loss: 0.0094\n",
      "Epoch 209/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 210/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0096 - val_loss: 0.0128\n",
      "Epoch 211/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 212/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 213/2000\n",
      "4/4 [==============================] - 3s 815ms/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 214/2000\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.0101 - val_loss: 0.0112\n",
      "Epoch 215/2000\n",
      "4/4 [==============================] - 2s 601ms/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 216/2000\n",
      "4/4 [==============================] - 3s 799ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 217/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 218/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 219/2000\n",
      "4/4 [==============================] - 3s 799ms/step - loss: 0.0098 - val_loss: 0.0111\n",
      "Epoch 220/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 221/2000\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 222/2000\n",
      "4/4 [==============================] - 3s 742ms/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 223/2000\n",
      "4/4 [==============================] - 3s 820ms/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 224/2000\n",
      "4/4 [==============================] - 3s 730ms/step - loss: 0.0104 - val_loss: 0.0124\n",
      "Epoch 225/2000\n",
      "4/4 [==============================] - 3s 730ms/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 226/2000\n",
      "4/4 [==============================] - 3s 734ms/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 227/2000\n",
      "4/4 [==============================] - 3s 778ms/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 228/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 229/2000\n",
      "4/4 [==============================] - 3s 816ms/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 230/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 231/2000\n",
      "4/4 [==============================] - 2s 591ms/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 232/2000\n",
      "4/4 [==============================] - 2s 622ms/step - loss: 0.0116 - val_loss: 0.0121\n",
      "Epoch 233/2000\n",
      "4/4 [==============================] - 3s 633ms/step - loss: 0.0101 - val_loss: 0.0096\n",
      "Epoch 234/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.0105 - val_loss: 0.0091\n",
      "Epoch 235/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 236/2000\n",
      "4/4 [==============================] - 2s 617ms/step - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 237/2000\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 0.0108 - val_loss: 0.0124\n",
      "Epoch 238/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 239/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 240/2000\n",
      "4/4 [==============================] - 3s 742ms/step - loss: 0.0107 - val_loss: 0.0126\n",
      "Epoch 241/2000\n",
      "4/4 [==============================] - 3s 754ms/step - loss: 0.0104 - val_loss: 0.0111\n",
      "Epoch 242/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 723ms/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 243/2000\n",
      "4/4 [==============================] - 3s 744ms/step - loss: 0.0099 - val_loss: 0.0109\n",
      "Epoch 244/2000\n",
      "4/4 [==============================] - 3s 774ms/step - loss: 0.0114 - val_loss: 0.0119\n",
      "Epoch 245/2000\n",
      "4/4 [==============================] - 3s 813ms/step - loss: 0.0110 - val_loss: 0.0091\n",
      "Epoch 246/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 247/2000\n",
      "4/4 [==============================] - 2s 571ms/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 248/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0099 - val_loss: 0.0131\n",
      "Epoch 249/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 250/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 251/2000\n",
      "4/4 [==============================] - 3s 628ms/step - loss: 0.0108 - val_loss: 0.0115\n",
      "Epoch 252/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 253/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0096 - val_loss: 0.0118\n",
      "Epoch 254/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 255/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 256/2000\n",
      "4/4 [==============================] - 2s 610ms/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 257/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 258/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 259/2000\n",
      "4/4 [==============================] - 2s 574ms/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 260/2000\n",
      "4/4 [==============================] - 3s 826ms/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 261/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 262/2000\n",
      "4/4 [==============================] - 3s 750ms/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 263/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.0098 - val_loss: 0.0118\n",
      "Epoch 264/2000\n",
      "4/4 [==============================] - 2s 616ms/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 265/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0106 - val_loss: 0.0124\n",
      "Epoch 266/2000\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 267/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 268/2000\n",
      "4/4 [==============================] - 3s 735ms/step - loss: 0.0102 - val_loss: 0.0111\n",
      "Epoch 269/2000\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 270/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 271/2000\n",
      "4/4 [==============================] - 3s 785ms/step - loss: 0.0112 - val_loss: 0.0115\n",
      "Epoch 272/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 273/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 274/2000\n",
      "4/4 [==============================] - 3s 796ms/step - loss: 0.0099 - val_loss: 0.0114\n",
      "Epoch 275/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0114 - val_loss: 0.0126\n",
      "Epoch 276/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 277/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0102 - val_loss: 0.0107\n",
      "Epoch 278/2000\n",
      "4/4 [==============================] - 2s 559ms/step - loss: 0.0113 - val_loss: 0.0117\n",
      "Epoch 279/2000\n",
      "4/4 [==============================] - 2s 604ms/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 280/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 281/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 282/2000\n",
      "4/4 [==============================] - 2s 625ms/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 283/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 284/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 285/2000\n",
      "4/4 [==============================] - 2s 621ms/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 286/2000\n",
      "4/4 [==============================] - 3s 863ms/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 287/2000\n",
      "4/4 [==============================] - 3s 787ms/step - loss: 0.0114 - val_loss: 0.0113\n",
      "Epoch 288/2000\n",
      "4/4 [==============================] - 3s 746ms/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 289/2000\n",
      "4/4 [==============================] - 3s 745ms/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 290/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 291/2000\n",
      "4/4 [==============================] - 2s 606ms/step - loss: 0.0102 - val_loss: 0.0107\n",
      "Epoch 292/2000\n",
      "4/4 [==============================] - 2s 573ms/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 293/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 294/2000\n",
      "4/4 [==============================] - 3s 625ms/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 295/2000\n",
      "4/4 [==============================] - 2s 601ms/step - loss: 0.0105 - val_loss: 0.0116\n",
      "Epoch 296/2000\n",
      "4/4 [==============================] - 2s 619ms/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 297/2000\n",
      "4/4 [==============================] - 2s 578ms/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 298/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 299/2000\n",
      "4/4 [==============================] - 2s 604ms/step - loss: 0.0099 - val_loss: 0.0103\n",
      "Epoch 300/2000\n",
      "4/4 [==============================] - 2s 604ms/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 301/2000\n",
      "4/4 [==============================] - 3s 738ms/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 302/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.0109 - val_loss: 0.0124\n",
      "Epoch 303/2000\n",
      "4/4 [==============================] - 2s 563ms/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 304/2000\n",
      "4/4 [==============================] - 2s 582ms/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 305/2000\n",
      "4/4 [==============================] - 3s 735ms/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 306/2000\n",
      "4/4 [==============================] - 2s 567ms/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 307/2000\n",
      "4/4 [==============================] - 2s 584ms/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 308/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 309/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0116 - val_loss: 0.0102\n",
      "Epoch 310/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 311/2000\n",
      "4/4 [==============================] - 3s 669ms/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 312/2000\n",
      "4/4 [==============================] - 3s 783ms/step - loss: 0.0108 - val_loss: 0.0115\n",
      "Epoch 313/2000\n",
      "4/4 [==============================] - 3s 638ms/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 314/2000\n",
      "4/4 [==============================] - 2s 548ms/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 315/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 316/2000\n",
      "4/4 [==============================] - 3s 844ms/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 317/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 318/2000\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 319/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 320/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 321/2000\n",
      "4/4 [==============================] - 2s 541ms/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 322/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 782ms/step - loss: 0.0108 - val_loss: 0.0130\n",
      "Epoch 323/2000\n",
      "4/4 [==============================] - 3s 743ms/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 324/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0115 - val_loss: 0.0107\n",
      "Epoch 325/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 326/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 327/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 328/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.0098 - val_loss: 0.0111\n",
      "Epoch 329/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 330/2000\n",
      "4/4 [==============================] - 3s 732ms/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 331/2000\n",
      "4/4 [==============================] - 4s 960ms/step - loss: 0.0100 - val_loss: 0.0096\n",
      "Epoch 332/2000\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 333/2000\n",
      "4/4 [==============================] - 3s 811ms/step - loss: 0.0098 - val_loss: 0.0109\n",
      "Epoch 334/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 335/2000\n",
      "4/4 [==============================] - 3s 734ms/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 336/2000\n",
      "4/4 [==============================] - 3s 758ms/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 337/2000\n",
      "4/4 [==============================] - 3s 801ms/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 338/2000\n",
      "4/4 [==============================] - 3s 745ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 339/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0112 - val_loss: 0.0116\n",
      "Epoch 340/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0106\n",
      "Epoch 341/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 342/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 343/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0125\n",
      "Epoch 344/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 345/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 346/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 347/2000\n",
      "4/4 [==============================] - 4s 890ms/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 348/2000\n",
      "4/4 [==============================] - 4s 968ms/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 349/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 350/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0090\n",
      "Epoch 351/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 352/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 353/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0119 - val_loss: 0.0125\n",
      "Epoch 354/2000\n",
      "4/4 [==============================] - 3s 865ms/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 355/2000\n",
      "4/4 [==============================] - 4s 966ms/step - loss: 0.0114 - val_loss: 0.0122\n",
      "Epoch 356/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0105 - val_loss: 0.0119\n",
      "Epoch 357/2000\n",
      "4/4 [==============================] - 4s 910ms/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 358/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 359/2000\n",
      "4/4 [==============================] - 4s 880ms/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 360/2000\n",
      "4/4 [==============================] - 4s 881ms/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 361/2000\n",
      "4/4 [==============================] - 4s 933ms/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 362/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 363/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 364/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 365/2000\n",
      "4/4 [==============================] - 4s 886ms/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 366/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 367/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0123\n",
      "Epoch 368/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0125\n",
      "Epoch 369/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 370/2000\n",
      "4/4 [==============================] - 3s 834ms/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 371/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 372/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 373/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0113 - val_loss: 0.0121\n",
      "Epoch 374/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0094\n",
      "Epoch 375/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 376/2000\n",
      "4/4 [==============================] - 4s 979ms/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 377/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 378/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0097 - val_loss: 0.0114\n",
      "Epoch 379/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0097 - val_loss: 0.0119\n",
      "Epoch 380/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 381/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 382/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 383/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 384/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 385/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 386/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 387/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 388/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 389/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 390/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 391/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 392/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0093 - val_loss: 0.0112\n",
      "Epoch 393/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 394/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 395/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0131\n",
      "Epoch 396/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0106\n",
      "Epoch 397/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 398/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 399/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 400/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 401/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 402/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0114\n",
      "Epoch 403/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0107\n",
      "Epoch 404/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0104\n",
      "Epoch 405/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 406/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 407/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 408/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 409/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0115\n",
      "Epoch 410/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0108\n",
      "Epoch 411/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 412/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0111\n",
      "Epoch 413/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 414/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 415/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0096 - val_loss: 0.0118\n",
      "Epoch 416/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 417/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0078\n",
      "Epoch 418/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 419/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 420/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0118\n",
      "Epoch 421/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 422/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 423/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 424/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 425/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0122\n",
      "Epoch 426/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0117\n",
      "Epoch 427/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0097\n",
      "Epoch 428/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 429/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 430/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 431/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0098\n",
      "Epoch 432/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0126\n",
      "Epoch 433/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 434/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 435/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 436/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 437/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 438/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 439/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0119\n",
      "Epoch 440/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0128\n",
      "Epoch 441/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 442/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 443/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 444/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 445/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0113\n",
      "Epoch 446/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 447/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 448/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 449/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 450/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 451/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0112\n",
      "Epoch 452/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0126\n",
      "Epoch 453/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 454/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 455/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 456/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0124\n",
      "Epoch 457/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 458/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0105\n",
      "Epoch 459/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 460/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 461/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 462/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 463/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 464/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 465/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0121\n",
      "Epoch 466/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 467/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0088\n",
      "Epoch 468/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 469/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 470/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 471/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 472/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 473/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 474/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0125\n",
      "Epoch 475/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 476/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0128\n",
      "Epoch 477/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 478/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0114\n",
      "Epoch 479/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 480/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 481/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 482/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 483/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 484/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0116\n",
      "Epoch 485/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 486/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 487/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 488/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0107\n",
      "Epoch 489/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 490/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0092\n",
      "Epoch 491/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 492/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 493/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 494/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0103\n",
      "Epoch 495/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0091\n",
      "Epoch 496/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0101\n",
      "Epoch 497/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0111\n",
      "Epoch 498/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 499/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0096 - val_loss: 0.0106\n",
      "Epoch 500/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 501/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 502/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 503/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0124\n",
      "Epoch 504/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 505/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0117\n",
      "Epoch 506/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 507/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 508/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 509/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0122\n",
      "Epoch 510/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0093 - val_loss: 0.0113\n",
      "Epoch 511/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 512/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0108\n",
      "Epoch 513/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 514/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0119 - val_loss: 0.0117\n",
      "Epoch 515/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 516/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 517/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0120\n",
      "Epoch 518/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 519/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 520/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0119\n",
      "Epoch 521/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0136\n",
      "Epoch 522/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 523/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 524/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 525/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 526/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 527/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 528/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0104\n",
      "Epoch 529/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0116\n",
      "Epoch 530/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 531/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0094\n",
      "Epoch 532/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 533/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0109\n",
      "Epoch 534/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 535/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 536/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0095\n",
      "Epoch 537/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 538/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0092\n",
      "Epoch 539/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 540/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 541/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 542/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 543/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 544/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 545/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 546/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 547/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 548/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0130\n",
      "Epoch 549/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0094 - val_loss: 0.0113\n",
      "Epoch 550/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 551/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 552/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0109\n",
      "Epoch 553/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 554/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 555/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0111\n",
      "Epoch 556/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0110\n",
      "Epoch 557/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 558/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0112\n",
      "Epoch 559/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0100\n",
      "Epoch 560/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0102\n",
      "Epoch 561/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0109\n",
      "Epoch 562/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 563/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 564/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 565/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 566/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0087\n",
      "Epoch 567/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 568/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 569/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0125\n",
      "Epoch 570/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 571/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0128\n",
      "Epoch 572/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 573/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 574/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 575/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0123\n",
      "Epoch 576/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0096 - val_loss: 0.0107\n",
      "Epoch 577/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0126\n",
      "Epoch 578/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 579/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0095 - val_loss: 0.0119\n",
      "Epoch 580/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0124\n",
      "Epoch 581/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 582/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0119\n",
      "Epoch 583/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 584/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 585/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 586/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 587/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 588/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 589/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 590/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 591/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 592/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 593/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 594/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0092\n",
      "Epoch 595/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 596/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 597/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0112\n",
      "Epoch 598/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0088\n",
      "Epoch 599/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 600/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0098\n",
      "Epoch 601/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0107\n",
      "Epoch 602/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 603/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 604/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 605/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0116\n",
      "Epoch 606/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 607/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 608/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 609/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 610/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 611/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0107\n",
      "Epoch 612/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0121\n",
      "Epoch 613/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 614/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 615/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 616/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 617/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 618/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 619/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 620/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 621/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 622/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 623/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 624/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0123\n",
      "Epoch 625/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 626/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 627/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 628/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 629/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 630/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 631/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0123 - val_loss: 0.0106\n",
      "Epoch 632/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0126\n",
      "Epoch 633/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 634/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0103\n",
      "Epoch 635/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 636/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 637/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 638/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0109\n",
      "Epoch 639/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 640/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0115\n",
      "Epoch 641/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0124\n",
      "Epoch 642/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 643/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 644/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0103\n",
      "Epoch 645/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 646/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0119\n",
      "Epoch 647/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 648/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 649/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0094\n",
      "Epoch 650/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0093\n",
      "Epoch 651/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 652/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 653/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 654/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 655/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0097\n",
      "Epoch 656/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0097\n",
      "Epoch 657/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 658/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0111\n",
      "Epoch 659/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0127\n",
      "Epoch 660/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0091\n",
      "Epoch 661/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0090\n",
      "Epoch 662/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 663/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 664/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 665/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 666/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 667/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 668/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0094 - val_loss: 0.0103\n",
      "Epoch 669/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 670/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 671/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0114\n",
      "Epoch 672/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0125\n",
      "Epoch 673/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 674/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 675/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 676/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 677/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 678/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 679/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0106\n",
      "Epoch 680/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 681/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 682/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0093 - val_loss: 0.0104\n",
      "Epoch 683/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 684/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0131\n",
      "Epoch 685/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 686/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 687/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 688/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 689/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 690/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 691/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 692/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 693/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0096\n",
      "Epoch 694/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0125\n",
      "Epoch 695/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0130\n",
      "Epoch 696/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 697/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 698/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 699/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0127\n",
      "Epoch 700/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 701/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 702/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 703/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 704/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 705/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0096\n",
      "Epoch 706/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 707/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0096 - val_loss: 0.0116\n",
      "Epoch 708/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 709/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 710/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0097 - val_loss: 0.0111\n",
      "Epoch 711/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0125\n",
      "Epoch 712/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 713/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 714/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 715/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 716/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 717/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 718/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 719/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 720/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0098 - val_loss: 0.0097\n",
      "Epoch 721/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0129\n",
      "Epoch 722/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0099\n",
      "Epoch 723/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 724/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 725/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0124\n",
      "Epoch 726/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 727/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0096\n",
      "Epoch 728/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0121\n",
      "Epoch 729/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 730/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0097\n",
      "Epoch 731/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0125\n",
      "Epoch 732/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 733/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 734/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 735/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 736/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 737/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0119\n",
      "Epoch 738/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 739/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0117\n",
      "Epoch 740/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 741/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0102\n",
      "Epoch 742/2000\n",
      "4/4 [==============================] - 4s 924ms/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 743/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0113\n",
      "Epoch 744/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 745/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 746/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0115\n",
      "Epoch 747/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 748/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 749/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 750/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 751/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0100\n",
      "Epoch 752/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 753/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 754/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0093\n",
      "Epoch 755/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 756/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 757/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 758/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 759/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 760/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 761/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 762/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 763/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0115\n",
      "Epoch 764/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 765/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 766/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0117\n",
      "Epoch 767/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 768/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 769/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0111\n",
      "Epoch 770/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 771/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 772/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 773/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0117\n",
      "Epoch 774/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0117\n",
      "Epoch 775/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0124\n",
      "Epoch 776/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 777/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0109\n",
      "Epoch 778/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 779/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 780/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 781/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 782/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 783/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 784/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 785/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 786/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 787/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0124\n",
      "Epoch 788/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 789/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 790/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 791/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 792/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0118\n",
      "Epoch 793/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 794/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0099 - val_loss: 0.0123\n",
      "Epoch 795/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 796/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 797/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0111\n",
      "Epoch 798/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 799/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 800/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0094\n",
      "Epoch 801/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 802/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0123\n",
      "Epoch 803/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 804/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 805/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 806/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 807/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 808/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 809/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0114\n",
      "Epoch 810/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 811/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 812/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 813/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0109\n",
      "Epoch 814/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 815/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0116\n",
      "Epoch 816/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 817/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 818/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0101\n",
      "Epoch 819/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 820/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0125\n",
      "Epoch 821/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0097 - val_loss: 0.0119\n",
      "Epoch 822/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 823/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 824/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0125\n",
      "Epoch 825/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 826/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 827/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 828/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 829/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 830/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 831/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0097 - val_loss: 0.0114\n",
      "Epoch 832/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0098\n",
      "Epoch 833/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0095\n",
      "Epoch 834/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 835/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 836/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0096 - val_loss: 0.0109\n",
      "Epoch 837/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0101\n",
      "Epoch 838/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 839/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0092\n",
      "Epoch 840/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 841/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 842/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 843/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 844/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 845/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0125\n",
      "Epoch 846/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0095 - val_loss: 0.0110\n",
      "Epoch 847/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 848/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0095\n",
      "Epoch 849/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0110\n",
      "Epoch 850/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 851/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0117\n",
      "Epoch 852/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 853/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 854/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0124\n",
      "Epoch 855/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0113\n",
      "Epoch 856/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 857/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0105\n",
      "Epoch 858/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 859/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0115\n",
      "Epoch 860/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 861/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0098 - val_loss: 0.0107\n",
      "Epoch 862/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 863/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 864/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 865/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 866/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 867/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 868/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0111\n",
      "Epoch 869/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0126\n",
      "Epoch 870/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 871/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 872/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 873/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0092\n",
      "Epoch 874/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0119\n",
      "Epoch 875/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0121 - val_loss: 0.0113\n",
      "Epoch 876/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0112\n",
      "Epoch 877/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 878/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0123\n",
      "Epoch 879/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 880/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0130\n",
      "Epoch 881/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0098\n",
      "Epoch 882/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0127\n",
      "Epoch 883/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 884/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 885/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0106\n",
      "Epoch 886/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 887/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 888/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0095 - val_loss: 0.0098\n",
      "Epoch 889/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 890/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 891/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0096 - val_loss: 0.0114\n",
      "Epoch 892/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 893/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 894/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0127\n",
      "Epoch 895/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 896/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 897/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 898/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0103\n",
      "Epoch 899/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 900/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0120\n",
      "Epoch 901/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0130\n",
      "Epoch 902/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 903/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0115\n",
      "Epoch 904/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 905/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 906/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0100\n",
      "Epoch 907/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 908/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 909/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 910/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0107\n",
      "Epoch 911/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0094\n",
      "Epoch 912/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0119\n",
      "Epoch 913/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 914/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 915/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 916/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0095\n",
      "Epoch 917/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 918/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 919/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 920/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 921/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 922/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0129\n",
      "Epoch 923/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0100\n",
      "Epoch 924/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0127\n",
      "Epoch 925/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 926/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 927/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 928/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 929/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 930/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0105\n",
      "Epoch 931/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 932/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 933/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0100\n",
      "Epoch 934/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 935/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 936/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 937/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 938/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 939/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 940/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 941/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0094 - val_loss: 0.0097\n",
      "Epoch 942/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 943/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0096\n",
      "Epoch 944/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 945/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 946/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 947/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 948/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 949/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 950/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0124\n",
      "Epoch 951/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0121\n",
      "Epoch 952/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0119\n",
      "Epoch 953/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 954/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 955/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0122\n",
      "Epoch 956/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 957/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 958/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 959/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0117\n",
      "Epoch 960/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0110\n",
      "Epoch 961/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 962/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0107\n",
      "Epoch 963/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0116\n",
      "Epoch 964/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 965/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 966/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 967/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0115\n",
      "Epoch 968/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 969/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0122\n",
      "Epoch 970/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0112\n",
      "Epoch 971/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0097 - val_loss: 0.0106\n",
      "Epoch 972/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 973/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 974/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 975/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0118\n",
      "Epoch 976/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 977/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 978/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 979/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 980/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 981/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 982/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 983/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 984/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0117\n",
      "Epoch 985/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 986/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 987/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0124\n",
      "Epoch 988/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0110\n",
      "Epoch 989/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0113\n",
      "Epoch 990/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 991/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 992/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 993/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 994/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0104\n",
      "Epoch 995/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0126\n",
      "Epoch 996/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 997/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 998/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 999/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 1000/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0102\n",
      "Epoch 1001/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0129\n",
      "Epoch 1002/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1003/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0125\n",
      "Epoch 1004/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0097 - val_loss: 0.0102\n",
      "Epoch 1005/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1006/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1007/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 1008/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 1009/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0113\n",
      "Epoch 1010/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 1011/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 1012/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0111\n",
      "Epoch 1013/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 1014/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 1015/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 1016/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0096 - val_loss: 0.0108\n",
      "Epoch 1017/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 1018/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 1019/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 1020/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0096\n",
      "Epoch 1021/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1022/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1023/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 1024/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0094\n",
      "Epoch 1025/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 1026/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 1027/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 1028/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 1029/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1030/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0114\n",
      "Epoch 1031/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 1032/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1033/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 1034/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 1035/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0118\n",
      "Epoch 1036/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0124\n",
      "Epoch 1037/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0117\n",
      "Epoch 1038/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0124\n",
      "Epoch 1039/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 1040/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 1041/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 1042/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 1043/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 1044/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 1045/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0105\n",
      "Epoch 1046/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 1047/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1048/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0127\n",
      "Epoch 1049/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 1050/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 1051/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0125\n",
      "Epoch 1052/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0119\n",
      "Epoch 1053/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 1054/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1055/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0105\n",
      "Epoch 1056/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1057/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 1058/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0117\n",
      "Epoch 1059/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0098 - val_loss: 0.0114\n",
      "Epoch 1060/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0093\n",
      "Epoch 1061/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1062/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 1063/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1064/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0101\n",
      "Epoch 1065/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 1066/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0118\n",
      "Epoch 1067/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0120\n",
      "Epoch 1068/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 1069/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 1070/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0095 - val_loss: 0.0115\n",
      "Epoch 1071/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0114\n",
      "Epoch 1072/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0105\n",
      "Epoch 1073/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1074/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1075/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0122\n",
      "Epoch 1076/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0092 - val_loss: 0.0115\n",
      "Epoch 1077/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 1078/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0114\n",
      "Epoch 1079/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 1080/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1081/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 1082/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0093 - val_loss: 0.0102\n",
      "Epoch 1083/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 1084/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0104\n",
      "Epoch 1085/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 1086/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 1087/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0116\n",
      "Epoch 1088/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 1089/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0105\n",
      "Epoch 1090/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1091/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 1092/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0117\n",
      "Epoch 1093/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0097 - val_loss: 0.0131\n",
      "Epoch 1094/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1095/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1096/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1097/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 1098/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0093\n",
      "Epoch 1099/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 1100/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 1101/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 1102/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 1103/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0096 - val_loss: 0.0107\n",
      "Epoch 1104/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0114\n",
      "Epoch 1105/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0101\n",
      "Epoch 1106/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 1107/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 1108/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 1109/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 1110/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 1111/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0116\n",
      "Epoch 1112/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 1113/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 1114/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 1115/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1116/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 1117/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 1118/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1119/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1120/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 1121/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 1122/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0111\n",
      "Epoch 1123/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0121 - val_loss: 0.0101\n",
      "Epoch 1124/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1125/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0094 - val_loss: 0.0109\n",
      "Epoch 1126/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1127/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 1128/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 1129/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 1130/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0121\n",
      "Epoch 1131/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0113\n",
      "Epoch 1132/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 1133/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0109\n",
      "Epoch 1134/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1135/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0120\n",
      "Epoch 1136/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0095\n",
      "Epoch 1137/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 1138/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0106\n",
      "Epoch 1139/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0096 - val_loss: 0.0114\n",
      "Epoch 1140/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0112\n",
      "Epoch 1141/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 1142/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 1143/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 1144/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 1145/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0132\n",
      "Epoch 1147/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0097\n",
      "Epoch 1148/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 1149/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 1150/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 1151/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 1152/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 1153/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0111\n",
      "Epoch 1154/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 1155/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 1156/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 1157/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1158/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0101\n",
      "Epoch 1159/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0116\n",
      "Epoch 1160/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1161/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0097 - val_loss: 0.0116\n",
      "Epoch 1162/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1163/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 1164/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1165/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 1166/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 1167/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 1168/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 1169/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0090\n",
      "Epoch 1170/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 1171/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 1172/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 1173/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0110\n",
      "Epoch 1174/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 1175/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 1176/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0118\n",
      "Epoch 1177/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0119 - val_loss: 0.0102\n",
      "Epoch 1178/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 1179/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0094\n",
      "Epoch 1180/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 1181/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0119\n",
      "Epoch 1182/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0096 - val_loss: 0.0118\n",
      "Epoch 1183/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1184/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1185/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 1186/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 1187/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0111\n",
      "Epoch 1188/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0118\n",
      "Epoch 1189/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1190/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 1191/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0096\n",
      "Epoch 1192/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 1193/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0111\n",
      "Epoch 1194/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1195/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 1196/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 1197/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 1198/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 1199/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1200/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 1201/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 1202/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 1203/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 1204/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 1205/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 1206/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 1207/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0097 - val_loss: 0.0109\n",
      "Epoch 1208/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0122\n",
      "Epoch 1209/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0114\n",
      "Epoch 1210/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 1211/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0094\n",
      "Epoch 1212/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 1213/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 1214/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0112\n",
      "Epoch 1215/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 1216/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 1217/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 1218/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0096\n",
      "Epoch 1219/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 1220/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0124\n",
      "Epoch 1221/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1222/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0106\n",
      "Epoch 1223/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 1224/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0125\n",
      "Epoch 1225/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 1226/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 1227/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0124\n",
      "Epoch 1228/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 1229/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 1230/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0098\n",
      "Epoch 1231/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0119\n",
      "Epoch 1232/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1233/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0093\n",
      "Epoch 1234/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 1235/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 1236/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 1237/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 1238/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0091\n",
      "Epoch 1239/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0108\n",
      "Epoch 1240/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 1241/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 1242/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0103\n",
      "Epoch 1243/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0118\n",
      "Epoch 1244/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 1245/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1246/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1247/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 1248/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 1249/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1250/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0118\n",
      "Epoch 1251/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0094 - val_loss: 0.0109\n",
      "Epoch 1252/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0092\n",
      "Epoch 1253/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 1254/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0112\n",
      "Epoch 1255/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1256/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0102\n",
      "Epoch 1257/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1258/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 1259/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0092\n",
      "Epoch 1260/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1261/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1262/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 1263/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0124\n",
      "Epoch 1264/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1265/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0095 - val_loss: 0.0118\n",
      "Epoch 1266/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 1267/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0101\n",
      "Epoch 1268/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0127\n",
      "Epoch 1269/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0100\n",
      "Epoch 1270/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0119\n",
      "Epoch 1271/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0117\n",
      "Epoch 1272/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 1273/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0096\n",
      "Epoch 1274/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 1275/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 1276/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 1277/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0122\n",
      "Epoch 1278/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 1279/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 1280/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 1281/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0127\n",
      "Epoch 1282/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1283/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 1284/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1285/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 1286/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0096\n",
      "Epoch 1287/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 1288/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 1289/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 1290/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 1291/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 1292/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0122\n",
      "Epoch 1293/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 1294/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 1295/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1296/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0128\n",
      "Epoch 1297/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 1298/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0111\n",
      "Epoch 1299/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 1300/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0099\n",
      "Epoch 1301/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1302/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0112\n",
      "Epoch 1303/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1304/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 1305/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0126\n",
      "Epoch 1306/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1307/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0130\n",
      "Epoch 1308/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1309/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 1310/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 1311/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0115\n",
      "Epoch 1312/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 1313/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 1314/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 1315/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 1316/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0115\n",
      "Epoch 1317/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0095\n",
      "Epoch 1318/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1319/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 1320/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0112\n",
      "Epoch 1321/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1322/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0097 - val_loss: 0.0120\n",
      "Epoch 1323/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0098\n",
      "Epoch 1324/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0119\n",
      "Epoch 1325/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1326/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 1327/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 1328/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0095\n",
      "Epoch 1329/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0118\n",
      "Epoch 1330/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 1331/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1332/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 1333/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0106\n",
      "Epoch 1334/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 1335/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0113\n",
      "Epoch 1336/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1337/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0100\n",
      "Epoch 1338/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0107\n",
      "Epoch 1339/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0101\n",
      "Epoch 1340/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0109\n",
      "Epoch 1341/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0098\n",
      "Epoch 1342/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0114\n",
      "Epoch 1343/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0103\n",
      "Epoch 1344/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0129\n",
      "Epoch 1345/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 1346/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 1347/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 1348/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 1349/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1350/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1351/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 1352/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0094 - val_loss: 0.0119\n",
      "Epoch 1353/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1354/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0109\n",
      "Epoch 1355/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1356/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 1357/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 1358/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 1359/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 1360/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1361/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0096 - val_loss: 0.0105\n",
      "Epoch 1362/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1363/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1364/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 1365/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 1366/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 1367/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1368/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 1369/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0098\n",
      "Epoch 1370/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 1371/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1372/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1373/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 1374/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 1375/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0128\n",
      "Epoch 1376/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1377/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0107\n",
      "Epoch 1378/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0118 - val_loss: 0.0100\n",
      "Epoch 1379/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 1380/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 1381/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 1382/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0113\n",
      "Epoch 1383/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1384/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0106\n",
      "Epoch 1385/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 1386/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0114\n",
      "Epoch 1387/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0111\n",
      "Epoch 1388/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 1389/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 1390/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0097 - val_loss: 0.0123\n",
      "Epoch 1391/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1392/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1393/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 1394/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1395/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 1396/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1397/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 1398/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0094\n",
      "Epoch 1399/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0124\n",
      "Epoch 1400/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 1401/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0087\n",
      "Epoch 1402/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1403/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0125\n",
      "Epoch 1404/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0116\n",
      "Epoch 1405/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 1406/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0091\n",
      "Epoch 1407/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 1408/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 1409/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 1410/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 1411/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 1412/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 1413/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0100\n",
      "Epoch 1414/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0119\n",
      "Epoch 1415/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0103\n",
      "Epoch 1416/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0118\n",
      "Epoch 1417/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1418/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 1419/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0117\n",
      "Epoch 1420/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 1421/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1422/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 1423/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 1424/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 1425/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0116\n",
      "Epoch 1426/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 1427/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 1428/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1429/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0094 - val_loss: 0.0102\n",
      "Epoch 1430/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 1431/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1432/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0117\n",
      "Epoch 1433/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 1434/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 1435/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 1436/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1437/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 1438/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 1439/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 1440/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0117\n",
      "Epoch 1441/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1442/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0131\n",
      "Epoch 1443/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 1444/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0123\n",
      "Epoch 1445/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0128\n",
      "Epoch 1446/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1447/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 1448/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0095 - val_loss: 0.0114\n",
      "Epoch 1449/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 1450/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0104\n",
      "Epoch 1451/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 1452/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 1453/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 1454/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 1455/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1456/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0127\n",
      "Epoch 1457/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 1458/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 1459/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 1460/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0121\n",
      "Epoch 1461/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0094 - val_loss: 0.0118\n",
      "Epoch 1462/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 1463/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0097 - val_loss: 0.0092\n",
      "Epoch 1464/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 1465/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1466/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0098\n",
      "Epoch 1467/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 1468/2000\n",
      "4/4 [==============================] - 3s 856ms/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 1469/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 1470/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1471/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0096 - val_loss: 0.0142\n",
      "Epoch 1472/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 1473/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1474/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 1475/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0117\n",
      "Epoch 1476/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1477/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1478/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0116\n",
      "Epoch 1479/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0122\n",
      "Epoch 1480/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0109\n",
      "Epoch 1481/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0119 - val_loss: 0.0111\n",
      "Epoch 1482/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0122\n",
      "Epoch 1483/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 1484/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0099\n",
      "Epoch 1485/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0129\n",
      "Epoch 1486/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 1487/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1488/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 1489/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0109\n",
      "Epoch 1490/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 1491/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0098 - val_loss: 0.0102\n",
      "Epoch 1492/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 1493/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 1494/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0118\n",
      "Epoch 1495/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 1496/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 1497/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0126\n",
      "Epoch 1498/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1499/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 1500/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0094\n",
      "Epoch 1501/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0117\n",
      "Epoch 1502/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 1503/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 1504/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 1505/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 1506/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 1507/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0112\n",
      "Epoch 1508/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1509/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 1510/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 1511/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0100\n",
      "Epoch 1512/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 1513/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1514/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 1515/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0100\n",
      "Epoch 1516/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 1517/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 1518/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 1519/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1520/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1521/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 1522/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0127\n",
      "Epoch 1523/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 1524/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1525/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0110\n",
      "Epoch 1526/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0114\n",
      "Epoch 1527/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 1528/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 1529/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 1530/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0138\n",
      "Epoch 1531/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0096 - val_loss: 0.0115\n",
      "Epoch 1532/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 1533/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0115\n",
      "Epoch 1534/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0105\n",
      "Epoch 1535/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 1536/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0129\n",
      "Epoch 1537/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1538/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 1539/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0098\n",
      "Epoch 1540/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1541/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0112\n",
      "Epoch 1542/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1543/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0095 - val_loss: 0.0112\n",
      "Epoch 1544/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 1545/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 1546/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0132\n",
      "Epoch 1547/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 1548/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 1549/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1550/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 1551/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1552/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1553/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0119\n",
      "Epoch 1554/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 1555/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1556/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0096\n",
      "Epoch 1557/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1558/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0118\n",
      "Epoch 1559/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 1560/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0119 - val_loss: 0.0108\n",
      "Epoch 1561/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0137\n",
      "Epoch 1562/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 1563/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0117\n",
      "Epoch 1564/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1565/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 1566/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 1567/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 1568/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 1569/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 1570/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0104\n",
      "Epoch 1571/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 1572/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 1573/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 1574/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0134\n",
      "Epoch 1575/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0098 - val_loss: 0.0117\n",
      "Epoch 1576/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 1577/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0123\n",
      "Epoch 1578/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0121\n",
      "Epoch 1579/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0097\n",
      "Epoch 1580/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0089\n",
      "Epoch 1581/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 1582/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 1583/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 1584/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 1585/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0118\n",
      "Epoch 1586/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 1587/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 1588/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0110\n",
      "Epoch 1589/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0105\n",
      "Epoch 1590/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 1591/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1592/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0100\n",
      "Epoch 1593/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1594/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0106\n",
      "Epoch 1595/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 1596/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0128\n",
      "Epoch 1597/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 1598/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 1599/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 1600/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 1601/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 1602/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 1603/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1604/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 1605/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0091\n",
      "Epoch 1606/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0123\n",
      "Epoch 1607/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 1608/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 1609/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 1610/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 1611/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 1612/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 1613/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1614/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0109\n",
      "Epoch 1615/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0116\n",
      "Epoch 1616/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 1617/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 1618/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0096\n",
      "Epoch 1619/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 1620/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1621/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1622/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0092\n",
      "Epoch 1623/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 1624/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 1625/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 1626/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0096\n",
      "Epoch 1627/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0096\n",
      "Epoch 1628/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0103\n",
      "Epoch 1629/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 1630/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0097 - val_loss: 0.0116\n",
      "Epoch 1631/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 1632/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 1633/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0124 - val_loss: 0.0107\n",
      "Epoch 1634/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 1635/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0118\n",
      "Epoch 1636/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 1637/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 1638/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0123\n",
      "Epoch 1639/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1640/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0131\n",
      "Epoch 1641/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1642/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0133\n",
      "Epoch 1643/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0118\n",
      "Epoch 1644/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 1645/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 1646/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 1647/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0116\n",
      "Epoch 1648/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 1649/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 1650/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0115\n",
      "Epoch 1651/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0097 - val_loss: 0.0112\n",
      "Epoch 1652/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0095 - val_loss: 0.0107\n",
      "Epoch 1653/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0090\n",
      "Epoch 1654/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0106\n",
      "Epoch 1655/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 1656/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1657/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1658/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 1659/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0096\n",
      "Epoch 1660/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 1661/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0105\n",
      "Epoch 1662/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 1663/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 1664/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0094 - val_loss: 0.0108\n",
      "Epoch 1665/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0123\n",
      "Epoch 1666/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 1667/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 1668/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 1669/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1670/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0140\n",
      "Epoch 1671/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0094\n",
      "Epoch 1672/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0097\n",
      "Epoch 1673/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0095 - val_loss: 0.0108\n",
      "Epoch 1674/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 1675/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 1676/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0103\n",
      "Epoch 1677/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0114\n",
      "Epoch 1678/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1679/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 1680/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1681/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1682/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 1683/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 1684/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 1685/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0127\n",
      "Epoch 1686/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 1687/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0125\n",
      "Epoch 1688/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 1689/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0111\n",
      "Epoch 1690/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 1691/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 1692/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0095\n",
      "Epoch 1693/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 1694/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 1695/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1696/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 1697/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 1698/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 1699/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1700/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 1701/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0109\n",
      "Epoch 1702/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 1703/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0106\n",
      "Epoch 1704/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 1705/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 1706/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 1707/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0101\n",
      "Epoch 1708/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 1709/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 1710/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1711/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0100\n",
      "Epoch 1712/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0107\n",
      "Epoch 1713/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 1714/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 1715/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0116\n",
      "Epoch 1716/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0120\n",
      "Epoch 1717/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0118 - val_loss: 0.0114\n",
      "Epoch 1718/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 1719/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0089\n",
      "Epoch 1720/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 1721/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0123\n",
      "Epoch 1722/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 1723/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0119 - val_loss: 0.0116\n",
      "Epoch 1724/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0097 - val_loss: 0.0115\n",
      "Epoch 1725/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 1726/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1727/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1728/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 1729/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0126\n",
      "Epoch 1730/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0124\n",
      "Epoch 1731/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0097 - val_loss: 0.0110\n",
      "Epoch 1732/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0095\n",
      "Epoch 1733/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1734/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 1735/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1736/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0092\n",
      "Epoch 1737/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0129\n",
      "Epoch 1738/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0096\n",
      "Epoch 1739/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0108\n",
      "Epoch 1740/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0107\n",
      "Epoch 1741/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0134\n",
      "Epoch 1742/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0099\n",
      "Epoch 1743/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 1744/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1745/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 1746/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 1747/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0115\n",
      "Epoch 1748/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0103\n",
      "Epoch 1749/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 1750/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 1751/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 1752/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 1753/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 1754/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 1755/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 1756/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0101\n",
      "Epoch 1757/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0118\n",
      "Epoch 1758/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0115\n",
      "Epoch 1759/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 1760/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 1761/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1762/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0090\n",
      "Epoch 1763/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 1764/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1765/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 1766/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 1767/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1768/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 1769/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1770/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 1771/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0096\n",
      "Epoch 1772/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0116\n",
      "Epoch 1773/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1774/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 1775/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 1776/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 1777/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 1778/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 1779/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1780/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0122 - val_loss: 0.0100\n",
      "Epoch 1781/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0108\n",
      "Epoch 1782/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0117\n",
      "Epoch 1783/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 1784/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0125\n",
      "Epoch 1785/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 1786/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0115\n",
      "Epoch 1787/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 1788/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0122\n",
      "Epoch 1789/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1790/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0117\n",
      "Epoch 1791/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 1792/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 1793/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 1794/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0113\n",
      "Epoch 1795/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 1796/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0088\n",
      "Epoch 1797/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 1798/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1799/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1800/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 1801/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 1802/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 1803/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0117\n",
      "Epoch 1804/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 1805/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 1806/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0095\n",
      "Epoch 1807/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 1808/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 1809/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 1810/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1811/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0117\n",
      "Epoch 1812/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 1813/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0096\n",
      "Epoch 1814/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0127\n",
      "Epoch 1815/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0121\n",
      "Epoch 1816/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 1817/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 1818/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 1819/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 1820/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1821/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 1822/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 1823/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0120 - val_loss: 0.0106\n",
      "Epoch 1824/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1825/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0126\n",
      "Epoch 1826/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 1827/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 1828/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 1829/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0108\n",
      "Epoch 1830/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0103\n",
      "Epoch 1831/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 1832/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0128\n",
      "Epoch 1833/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 1834/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 1835/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0129\n",
      "Epoch 1836/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 1837/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 1838/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 1839/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0114\n",
      "Epoch 1840/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 1841/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 1842/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 1843/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 1844/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0121\n",
      "Epoch 1845/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1846/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 1847/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 1848/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 1849/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0122\n",
      "Epoch 1850/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 1851/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0122\n",
      "Epoch 1852/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 1853/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0123\n",
      "Epoch 1854/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 1855/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0115\n",
      "Epoch 1856/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 1857/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0118\n",
      "Epoch 1858/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 1859/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 1860/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0119\n",
      "Epoch 1861/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0101\n",
      "Epoch 1862/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 1863/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 1864/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 1865/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 1866/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0101\n",
      "Epoch 1867/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0095 - val_loss: 0.0115\n",
      "Epoch 1868/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 1869/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 1870/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 1871/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 1872/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 1873/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1874/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0122\n",
      "Epoch 1875/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 1876/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1877/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 1878/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 1879/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 1880/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 1881/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 1882/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0112\n",
      "Epoch 1883/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0110\n",
      "Epoch 1884/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1885/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 1886/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1887/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 1888/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 1889/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 1890/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0116\n",
      "Epoch 1891/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 1892/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0095 - val_loss: 0.0108\n",
      "Epoch 1893/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1894/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 1895/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 1896/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0122\n",
      "Epoch 1897/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 1898/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0113\n",
      "Epoch 1899/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 1900/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 1901/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1902/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 1903/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0113\n",
      "Epoch 1904/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0093 - val_loss: 0.0114\n",
      "Epoch 1905/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0114\n",
      "Epoch 1906/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 1907/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 1908/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0123\n",
      "Epoch 1909/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0107\n",
      "Epoch 1910/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 1911/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 1912/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 1913/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0114\n",
      "Epoch 1914/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0130\n",
      "Epoch 1915/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 1916/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 1917/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 1918/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 1919/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1920/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1921/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0109\n",
      "Epoch 1922/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1923/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0102\n",
      "Epoch 1924/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 1925/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 1926/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0128\n",
      "Epoch 1927/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 1928/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 1929/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0095 - val_loss: 0.0088\n",
      "Epoch 1930/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1931/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0116\n",
      "Epoch 1932/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0117\n",
      "Epoch 1933/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0115\n",
      "Epoch 1934/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0115 - val_loss: 0.0100\n",
      "Epoch 1935/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 1936/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0096 - val_loss: 0.0112\n",
      "Epoch 1937/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0099 - val_loss: 0.0111\n",
      "Epoch 1938/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 1939/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0104\n",
      "Epoch 1940/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0125\n",
      "Epoch 1941/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0099\n",
      "Epoch 1942/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 1943/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0090\n",
      "Epoch 1944/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 1945/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 1946/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 1947/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 1948/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 1949/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 1950/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0106\n",
      "Epoch 1951/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0109\n",
      "Epoch 1952/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 1953/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 1954/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 1955/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0125\n",
      "Epoch 1956/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 1957/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0095 - val_loss: 0.0111\n",
      "Epoch 1958/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 1959/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0117\n",
      "Epoch 1960/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0124\n",
      "Epoch 1961/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0094\n",
      "Epoch 1962/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1963/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 1964/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 1965/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0128\n",
      "Epoch 1966/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0124\n",
      "Epoch 1967/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0132\n",
      "Epoch 1968/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 1969/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1970/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1971/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 1972/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0114\n",
      "Epoch 1973/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0105\n",
      "Epoch 1974/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 1975/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 1976/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 1977/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 1978/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1979/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1980/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 1981/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1982/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0097 - val_loss: 0.0103\n",
      "Epoch 1983/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0121\n",
      "Epoch 1984/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1985/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0090 - val_loss: 0.0100\n",
      "Epoch 1986/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0132\n",
      "Epoch 1987/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 1988/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0119 - val_loss: 0.0106\n",
      "Epoch 1989/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 1990/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0105\n",
      "Epoch 1991/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 1992/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0125\n",
      "Epoch 1993/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 1994/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 1995/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 1996/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 1997/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 1998/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1999/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 2000/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0100\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(tdg, callbacks=[mc,tb], initial_epoch=0\n",
    "                           ,steps_per_epoch=train_steps_per_epoch\n",
    "                           ,validation_data=vdg\n",
    "                           ,validation_steps=val_steps_per_epoch\n",
    "                           ,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# acc = hist.history['acc']\n",
    "# loss = hist.history['loss']\n",
    "\n",
    "# # Create count of the number of epochs\n",
    "# epoch_count = range(1, len(acc) + 1)\n",
    "\n",
    "# # Visualize loss history\n",
    "# # plt.plot(epoch_count, acc, 'b-')\n",
    "# fig, ax = plt.subplots(ncols=2,sharex=True)\n",
    "# ax[0].plot(epoch_count, loss, 'r--')\n",
    "# ax[0].legend(['Loss'])\n",
    "# ax[0].set_xlabel('Epoch')\n",
    "# ax[0].set_ylabel('Loss')\n",
    "# ax[1].plot(epoch_count, acc, 'b-')\n",
    "# ax[1].legend(['Accuracy'])\n",
    "# ax[1].set_xlabel('Epoch')\n",
    "# ax[1].set_xlabel('Accuracy')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res[['acc','val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutate SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshaikh2/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `evaluate_generator` call to the Keras 2 API: `evaluate_generator(<generator..., use_multiprocessing=True, steps=5)`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.010333957150578499"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('weights/dnf300_sa_sent_hd_vector_gl.hdf5')\n",
    "model.evaluate_generator(test_dg,steps=5,pickle_safe = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(test_dg)\n",
    "pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"george soros: trump will win popular vote by a landslide but clinton victory a 'done deal'\",\n",
       "       'reddit users declare war on hillarys paid internet trolls',\n",
       "       'erdoan: us, the founder of isis',\n",
       "       'us officials see no link between trump and russia',\n",
       "       'assange confirms: wikileaks didnt get emails from russian govt',\n",
       "       'us threatens military hacks on russias electric, communications grids over election',\n",
       "       'fbi director comeys leaked memo explains why hes reopening the clinton email case',\n",
       "       'trump accuses obama, hillary clinton of founding daesh',\n",
       "       'fbi director comeys leaked memo explains why hes reopening the clinton email case',\n",
       "       'hes never sold an original painting until nowand this ones going in the white house',\n",
       "       \"physician confirms hillary clinton has parkinson's disease\",\n",
       "       \"hillary clinton wore 'secret earpiece' during commander-in-chief forum\",\n",
       "       'erdoan: us, the founder of isis',\n",
       "       \"doj's loretta lynch tried to squash comey's letter to congress\",\n",
       "       'lol! british wife of lib actor who said: there will never be a president donald trumpwarns americans about president-elect trump [video]',\n",
       "       'lol! british wife of lib actor who said: there will never be a president donald trumpwarns americans about president-elect trump [video]',\n",
       "       'reddit users declare war on hillarys paid internet trolls',\n",
       "       'hillary clinton used hand signals to rig debate?',\n",
       "       \"hillary clinton cut her tax bill by 'donating' $1 million to herself via the clinton foundation?\",\n",
       "       'leaked 2013 trump tax return shows he paid over 40 million in taxes',\n",
       "       'clinton received debate questions week before debate',\n",
       "       \"fantastic! trump's 7 point plan to reform healthcare begins with a bombshell!  100percentfedup.com\",\n",
       "       \"hillary clinton in 2013: 'i would like to see people like donald trump run for office\",\n",
       "       'isis leader calls for american muslim voters to support hillary clinton',\n",
       "       \"physician confirms hillary clinton has parkinson's disease\",\n",
       "       'hes never sold an original painting until nowand this ones going in the white house',\n",
       "       'pentagon seeks another $6 billion for overseas troop deployments',\n",
       "       'obama declares his family will move to canada if trump is elected',\n",
       "       'top aide: hillary still not perfect in her head, wikileaks',\n",
       "       'president obama confirms he will refuse to leave office if trump is elected',\n",
       "       \"hillary clinton in 2013: 'i would like to see people like donald trump run for office\",\n",
       "       \"physician confirms hillary clinton has parkinson's disease\",\n",
       "       'hillarys (islamic) america is already here where muslim no-go zones are popping up all over michiganistan',\n",
       "       \"clinton camp demands 'compliant citizenry' for master plan\",\n",
       "       'us officials see no link between trump and russia',\n",
       "       \"george soros: trump will win popular vote by a landslide but clinton victory a 'done deal'\",\n",
       "       'kremlin: putin congratulates trump, hopes to work together major issues',\n",
       "       \"hillary clinton cut her tax bill by 'donating' $1 million to herself via the clinton foundation?\",\n",
       "       'wikileaks confirms hillary sold weapons to isis... then drops another bombshell! breaking news',\n",
       "       \"hillary clinton's 'sudden move' of $1.8 billion to qatar central bank stuns financial world\",\n",
       "       'leaked 2013 trump tax return shows he paid over 40 million in taxes',\n",
       "       'fbi agent suspected in hillary email leaks found dead in apparent murder-suicide',\n",
       "       'hillary personally ordered donald duck troll campaign',\n",
       "       'hillary personally ordered donald duck troll campaign',\n",
       "       'hillary clintons sudden move of $1.8 billion to qatar central bank stuns financial world',\n",
       "       'fbi agent suspected in hillary email leaks found dead in apparent murder-suicide',\n",
       "       'top aide: hillary still not perfect in her head, wikileaks',\n",
       "       'pentagon seeks another $6 billion for overseas troop deployments',\n",
       "       'department of homeland security chairman officially indicts hillary clinton of treason',\n",
       "       \"hillary clinton's 'sudden move' of $1.8 billion to qatar central bank stuns financial world\"],\n",
       "      dtype='<U139')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hillary personally ordered donald duck troll campaign'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_idx = np.random.randint(0,50)\n",
    "display(x['headline'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hillary Clinton personally ordered a consultant to use a nonprofit group to troll the Trump campaign with a Donald Duck mascot, according to the Democratic operatives who say they arranged it with a nonprofit organization.',\n",
       " 'When Breitbart News Washington political editor, Matthew Boyle, confronted Mook about Creamer and his firm in the spin room after the third presidential debate, Mook claimed: Theyve never worked for our campaign.',\n",
       " 'When asked if Clinton had ever discussed the controversial political operations with Creamer directly, Mook replied: I dont think so.',\n",
       " 'Now, however, OKeefe and Project Veritas have released video of Creamer claiming that Clinton directly approved one of his more bizarre plans  an effort to attract media attention and incite violence by dressing an activist in a Donald Duck costume and sending that activist into Trump events, emphasizing the argument that Trump was ducking releasing his tax returns.',\n",
       " 'The action, if true, would be a black-letter violation of federal election law, which prohibits presidential campaigns from coordinating activities with outside groups that can collect unlimited dark money from contributors  and dont pay taxes on what they collect.',\n",
       " 'Project Veritas Action video footage shows Robert Creamer, a convicted felon who was forced out of his executive role at the liberal consultancy Democracy Partners, saying Clinton chose the duck stunt.',\n",
       " 'In the end, it was the candidate, Hillary Clinton, the future president of the United States, who wanted ducks on the ground.',\n",
       " 'So by God we would get ducks on the ground, Creamer says in the video.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['sentences'][test_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 35, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 35, 16)       14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 35, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 35, 32)       1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 35, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 35, 32)       128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 35, 128)      0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 35, 256)      98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 35, 256)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 35, 256)      1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca1 (CrossAttention_gl)         [(None, 35, 256), (1 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,322\n",
      "Trainable params: 350,234\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 35, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 35, 16)       14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 35, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 35, 32)       1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 35, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 35, 32)       128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 35, 128)      0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 35, 256)      98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 35, 256)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 35, 256)      1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca2 (CrossAttention_gl)         [(None, 35, 256), (1 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,322\n",
      "Trainable params: 350,234\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 35, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 35, 16)       14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 35, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 35, 32)       1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 35, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 35, 32)       128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 35, 128)      0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 35, 256)      98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 35, 256)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 35, 256)      1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca3 (CrossAttention_gl)         [(None, 35, 256), (1 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,322\n",
      "Trainable params: 350,234\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 35, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 35, 16)       14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 35, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 35, 32)       1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 35, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 35, 32)       128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention_gl)          [(None, 35, 32), (35 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 35, 128)      0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 35, 256)      98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 35, 256)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 35, 256)      1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca4 (CrossAttention_gl)         [(None, 35, 256), (1 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,322\n",
      "Trainable params: 350,234\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Model(model.inputs,model.get_layer(name='ca1').output)\n",
    "model_2 = Model(model.inputs,model.get_layer(name='ca2').output)\n",
    "model_3 = Model(model.inputs,model.get_layer(name='ca3').output)\n",
    "model_4 = Model(model.inputs,model.get_layer(name='ca4').output)\n",
    "model_1.summary()\n",
    "model_2.summary()\n",
    "model_3.summary()\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, b1, g1 = model_1.predict(x)\n",
    "_, b2, g2 = model_2.predict(x)\n",
    "_, b3, g3 = model_3.predict(x)\n",
    "_, b4, g4 = model_4.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1,g2,g3,g4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b1+b2+b3+b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 6, 5, 4, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_N = 5\n",
    "t = b[test_idx][0][:len(x['sentences'][test_idx])].argsort()[-best_N:][::-1]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x['sentences'][test_idx]))\n",
    "b[test_idx][0][:len(x['sentences'][test_idx])].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hillary personally ordered donald duck troll campaign'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Hillary Clinton personally ordered a consultant to use a nonprofit group to troll the Trump campaign with a Donald Duck mascot, according to the Democratic operatives who say they arranged it with a nonprofit organization.',\n",
       " 'Now, however, OKeefe and Project Veritas have released video of Creamer claiming that Clinton directly approved one of his more bizarre plans  an effort to attract media attention and incite violence by dressing an activist in a Donald Duck costume and sending that activist into Trump events, emphasizing the argument that Trump was ducking releasing his tax returns.',\n",
       " 'The action, if true, would be a black-letter violation of federal election law, which prohibits presidential campaigns from coordinating activities with outside groups that can collect unlimited dark money from contributors  and dont pay taxes on what they collect.',\n",
       " 'In the end, it was the candidate, Hillary Clinton, the future president of the United States, who wanted ducks on the ground.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x['headline'][test_idx])\n",
    "display(x['claims'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 : So by God we would get ducks on the ground, Creamer says in the video.\n",
      "6 : In the end, it was the candidate, Hillary Clinton, the future president of the United States, who wanted ducks on the ground.\n",
      "5 : Project Veritas Action video footage shows Robert Creamer, a convicted felon who was forced out of his executive role at the liberal consultancy Democracy Partners, saying Clinton chose the duck stunt.\n",
      "4 : The action, if true, would be a black-letter violation of federal election law, which prohibits presidential campaigns from coordinating activities with outside groups that can collect unlimited dark money from contributors  and dont pay taxes on what they collect.\n",
      "3 : Now, however, OKeefe and Project Veritas have released video of Creamer claiming that Clinton directly approved one of his more bizarre plans  an effort to attract media attention and incite violence by dressing an activist in a Donald Duck costume and sending that activist into Trump events, emphasizing the argument that Trump was ducking releasing his tax returns.\n"
     ]
    }
   ],
   "source": [
    "for s in t:\n",
    "    if s>=len(x['sentences'][test_idx]):continue\n",
    "    print(s,':',x['sentences'][test_idx][s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x['sentences'][test_idx]))\n",
    "h_s_attended_vector = b[test_idx][0][:len(x['sentences'][test_idx])]\n",
    "h_s_attended_vector.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h_s_attended_vector = pd.DataFrame(h_s_attended_vector)\n",
    "\n",
    "\n",
    "xw = df_h_s_attended_vector.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "xw_scaled = min_max_scaler.fit_transform(xw)\n",
    "df_h_s_attended_vector = pd.DataFrame(xw_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd3d6bbb7f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(2.0,7.0)})\n",
    "sns.heatmap(df_h_s_attended_vector, annot=True, cmap='YlGnBu', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 35, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 35, 16)            14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 35, 32)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 35, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 35, 32)            128       \n",
      "_________________________________________________________________\n",
      "sa1 (SelfAttention_gl)       [(None, 35, 32), (35, 35) 2378      \n",
      "=================================================================\n",
      "Total params: 18,490\n",
      "Trainable params: 18,426\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 35, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 35, 16)            14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 35, 32)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 35, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 35, 32)            128       \n",
      "_________________________________________________________________\n",
      "sa2 (SelfAttention_gl)       [(None, 35, 32), (35, 35) 2378      \n",
      "=================================================================\n",
      "Total params: 18,490\n",
      "Trainable params: 18,426\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 35, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 35, 16)            14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 35, 32)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 35, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 35, 32)            128       \n",
      "_________________________________________________________________\n",
      "sa3 (SelfAttention_gl)       [(None, 35, 32), (35, 35) 2378      \n",
      "=================================================================\n",
      "Total params: 18,490\n",
      "Trainable params: 18,426\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 35, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 35, 16)            14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 35, 32)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 35, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 35, 32)            128       \n",
      "_________________________________________________________________\n",
      "sa4 (SelfAttention_gl)       [(None, 35, 32), (35, 35) 2378      \n",
      "=================================================================\n",
      "Total params: 18,490\n",
      "Trainable params: 18,426\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_s1 = Model(model.inputs,model.get_layer(name='sa1').output)\n",
    "model_s2 = Model(model.inputs,model.get_layer(name='sa2').output)\n",
    "model_s3 = Model(model.inputs,model.get_layer(name='sa3').output)\n",
    "model_s4 = Model(model.inputs,model.get_layer(name='sa4').output)\n",
    "model_s1.summary()\n",
    "model_s2.summary()\n",
    "model_s3.summary()\n",
    "model_s4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sb1, sg1 = model_s1.predict([x['sentence_vectors'],x['input_headline_vector']])\n",
    "_, sb2, sg2 = model_s2.predict(x)\n",
    "_, sb3, sg3 = model_s3.predict(x)\n",
    "_, sb4, sg4 = model_s4.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg1,sg2, sg3, sg4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb = sb1[test_idx]+sb2[test_idx]+sb3[test_idx]+sb4[test_idx]\n",
    "sb = sb[:len(x['sentences'][test_idx]),:len(x['sentences'][test_idx])]\n",
    "\n",
    "sb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb = pd.DataFrame(sb)\n",
    "\n",
    "\n",
    "# zx = df_sb.values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# zx_scaled = min_max_scaler.fit_transform(zx)\n",
    "# df_sb = pd.DataFrame(zx_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd3e2ac96d8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(14.0,12.0)})\n",
    "sns.heatmap(pd.DataFrame(df_sb),annot=True,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hillary Clinton personally ordered a consultant to use a nonprofit group to troll the Trump campaign with a Donald Duck mascot, according to the Democratic operatives who say they arranged it with a nonprofit organization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When Breitbart News Washington political editor, Matthew Boyle, confronted Mook about Creamer and his firm in the spin room after the third presidential debate, Mook claimed: Theyve never worked for our campaign.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When asked if Clinton had ever discussed the controversial political operations with Creamer directly, Mook replied: I dont think so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Now, however, OKeefe and Project Veritas have released video of Creamer claiming that Clinton directly approved one of his more bizarre plans  an effort to attract media attention and incite violence by dressing an activist in a Donald Duck costume and sending that activist into Trump events, emphasizing the argument that Trump was ducking releasing his tax returns.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The action, if true, would be a black-letter violation of federal election law, which prohibits presidential campaigns from coordinating activities with outside groups that can collect unlimited dark money from contributors  and dont pay taxes on what they collect.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Project Veritas Action video footage shows Robert Creamer, a convicted felon who was forced out of his executive role at the liberal consultancy Democracy Partners, saying Clinton chose the duck stunt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In the end, it was the candidate, Hillary Clinton, the future president of the United States, who wanted ducks on the ground.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>So by God we would get ducks on the ground, Creamer says in the video.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                      0\n",
       "0  Hillary Clinton personally ordered a consultant to use a nonprofit group to troll the Trump campaign with a Donald Duck mascot, according to the Democratic operatives who say they arranged it with a nonprofit organization.                                                                                                                                                    \n",
       "1  When Breitbart News Washington political editor, Matthew Boyle, confronted Mook about Creamer and his firm in the spin room after the third presidential debate, Mook claimed: Theyve never worked for our campaign.                                                                                                                                                            \n",
       "2  When asked if Clinton had ever discussed the controversial political operations with Creamer directly, Mook replied: I dont think so.                                                                                                                                                                                                                                            \n",
       "3  Now, however, OKeefe and Project Veritas have released video of Creamer claiming that Clinton directly approved one of his more bizarre plans  an effort to attract media attention and incite violence by dressing an activist in a Donald Duck costume and sending that activist into Trump events, emphasizing the argument that Trump was ducking releasing his tax returns.\n",
       "4  The action, if true, would be a black-letter violation of federal election law, which prohibits presidential campaigns from coordinating activities with outside groups that can collect unlimited dark money from contributors  and dont pay taxes on what they collect.                                                                                                       \n",
       "5  Project Veritas Action video footage shows Robert Creamer, a convicted felon who was forced out of his executive role at the liberal consultancy Democracy Partners, saying Clinton chose the duck stunt.                                                                                                                                                                           \n",
       "6  In the end, it was the candidate, Hillary Clinton, the future president of the United States, who wanted ducks on the ground.                                                                                                                                                                                                                                                      \n",
       "7  So by God we would get ducks on the ground, Creamer says in the video.                                                                                                                                                                                                                                                                                                             "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x['sentences'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
